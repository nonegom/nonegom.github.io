---
title:  "[머신러닝] 지도학습 - 6.2. 비대칭 데이터 문제"
excerpt: "비대칭 데이터가 주어졌을 경우 데이터 분석을 위해 데이터를 가공하는 방법"

categories:
  - MachinLearning
tags:
  - SupervisedLearning
  - 10월
toc: true
toc_sticky: true
toc_label: 페이지 목차
use_math: true
---
> 아래 포스팅은 기존 수업의 복습 차원에서 올리는 포스팅입니다. 따라서 세부적인 수학적 정리가 생략된 부분이 있습니다. 따라서 좀 더 구체적인 정보나 원하시면 [데이터 사이언스 스쿨 사이트](https://datascienceschool.net/03%20machine%20learning/14.02%20%EB%B9%84%EB%8C%80%EC%B9%AD%20%EB%8D%B0%EC%9D%B4%ED%84%B0%20%EB%AC%B8%EC%A0%9C.html)를 참고 부탁드립니다. 특히 아래 코드를 이용한 시각화 그래프 코드와 모습을 보고 싶으시면 링크 확인부탁드립니다.  

## 0. 비대칭 데이터 문제
데이터 클래스 비율이 너무 차이가 나면(highly-imbalanced data) 모형의 성능판별이 어려워진다. 왜냐하면 단순히 우세한 클레스를 택하는 모형의 정확도가 높아지기 때문에 모형의 성능판별이 어려워진다. 정확도(accuracy)가 높아도 데이터의 갯수가 적은 클래스의 재현율(recall-rate)이 급격히 작아지는 현상이 발생할 수 있다.
 
- 희귀병을 찾는 경우, 정상인 데이터보다 비정상인 데이터가 더 적기 때문에 판별이 어려운 경우 등에 사용된다. 

### 사용하는 샘플 데이터 및 시각화 함수

```py
from sklearn.datasets import *
from sklearn.model_selection import train_test_split
from sklearn.metrics import recall_score
from sklearn.svm import SVC

def classification_result(n0, n1, title=""):
    rv1 = sp.stats.multivariate_normal([-1, 0], [[1, 0], [0, 1]])
    rv2 = sp.stats.multivariate_normal([+1, 0], [[1, 0], [0, 1]])
    X0 = rv1.rvs(n0, random_state=0)
    X1 = rv2.rvs(n1, random_state=0)
    X = np.vstack([X0, X1])
    y = np.hstack([np.zeros(n0), np.ones(n1)])

    x1min = -4; x1max = 4
    x2min = -2; x2max = 2
    xx1 = np.linspace(x1min, x1max, 1000)
    xx2 = np.linspace(x2min, x2max, 1000)
    X1, X2 = np.meshgrid(xx1, xx2)
    
    plt.contour(X1, X2, rv1.pdf(np.dstack([X1, X2])), levels=[0.05], linestyles="dashed")
    plt.contour(X1, X2, rv2.pdf(np.dstack([X1, X2])), levels=[0.05], linestyles="dashed")
    
    # SVC모델(선형 커널) 사용 및 예측
    model = SVC(kernel="linear", C=1e4, random_state=0).fit(X, y)
    Y = np.reshape(model.predict(np.array([X1.ravel(), X2.ravel()]).T), X1.shape)
    plt.scatter(X[y == 0, 0], X[y == 0, 1], marker='x', label="0 클래스")
    plt.scatter(X[y == 1, 0], X[y == 1, 1], marker='o', label="1 클래스")
    plt.contour(X1, X2, Y, colors='k', levels=[0.5])
    y_pred = model.predict(X)
    plt.xlim(-4, 4)
    plt.ylim(-3, 3)
    plt.xlabel("x1")
    plt.ylabel("x2")
    plt.title(title)
    return model, X, y, y_pred

plt.subplot(121)
model1, X1, y1, y_pred1 = classification_result(200, 200, "대칭 데이터 (5:5)")

plt.subplot(122)
model2, X2, y2, y_pred2 = classification_result(200, 20, "비대칭 데이터 (9:1)")
plt.tight_layout()
plt.show()
```
![](assets/images/Supervised14_1.png)
- 판별선이 다수방향에서 소수방향 쪽으로 밀려나게 된다.

```py
from sklearn.metrics import roc_curve, confusion_matrix
fpr1, tpr1, thresholds1 = roc_curve(y1, model1.decision_function(X1))
fpr2, tpr2, thresholds2 = roc_curve(y2, model2.decision_function(X2))

c1 = confusion_matrix(y1, y_pred1, labels=[1, 0])
c2 = confusion_matrix(y2, y_pred2, labels=[1, 0])
r1 = c1[0, 0] / (c1[0, 0] + c1[0, 1])
r2 = c2[0, 0] / (c2[0, 0] + c2[0, 1])
f1 = c1[1, 0] / (c1[1, 0] + c1[1, 1])
f2 = c2[1, 0] / (c2[1, 0] + c2[1, 1])

plt.plot(fpr1, tpr1, ':', label="대칭")
plt.plot(fpr2, tpr2, '-', label="비대칭")
plt.plot([f1], [r1], 'ro')
plt.plot([f2], [r2], 'ro')
plt.legend()
plt.xlabel('Fall-Out')
plt.ylabel('Recall')
plt.title('ROC 커브')
plt.show()
```
![](/assets/images/Supervised14_2.png)
- 비대칭 데이터의 ROC커브의 recall(재현율)값이 너무 낮다. 따라서 threshold(기준값)을 낮춤으로써 기준선을 당길 수도 있다.

### 해결방법
비대칭 데이터는 다수 클래스 데이터에서 일부만 사용하거나, 소수 클래스 데이터를 증가시키는 방법을 사용해서 데이터 비율을 맞추면 정밀도(precision)가 향상된다.

- **오버샘플링(Over-Sampling)**
- **언더샘플링(Under-Sampling)**
- **복합샘플링(Combining Over-and Under-Sampling)**

## 1. 언더 샘플링
다수 클래스 데이터에서 많이 있는 데이터를 없애고 일부만 사용하는 방법

- RandomUnderSampler 
- TomekLinks 
- CondensedNearestNeighbour 
- OneSidedSelection
- EditedNearestNeighbours  
- NeighbourhoodCleaningRule 


### 샘플 데이터

> 이하 그래프 시각화 코드는 생략 (위 출처 참고)

```py

n0 = 200; n1 = 20
rv1 = sp.stats.multivariate_normal([-1, 0], [[1, 0], [0, 1]])
rv2 = sp.stats.multivariate_normal([+1, 0], [[1, 0], [0, 1]])
X0 = rv1.rvs(n0, random_state=0)
X1 = rv2.rvs(n1, random_state=0)
X_imb = np.vstack([X0, X1])
y_imb = np.hstack([np.zeros(n0), np.ones(n1)])

x1min = -4; x1max = 4
x2min = -2; x2max = 2
xx1 = np.linspace(x1min, x1max, 1000)
xx2 = np.linspace(x2min, x2max, 1000)
X1, X2 = np.meshgrid(xx1, xx2)

def classification_result2(X, y, title=""):

    ##### 그래프 시각화 코드 생략 #####
    # SVC 모델 사용
    model = SVC(kernel="linear", C=1e4, random_state=0).fit(X, y)
    ##### 그래프 시각화 코드 생략 #####
```


### imbalanced-learn 패키지
imbalabced data 문제를 해결하기 위한 다양한 샘플링 방법을 구현한 파이썬 패키지  
`pip install -U imbalanced-learn`

### Ramdom Under-Sampler
무작위로 데이터를 없애는 단순 샘플링
```py
# 샘플링 수행
X_samp, y_samp = RandomUnderSampler(random_state=0).fit_sample(X_imb, y_imb)

# 그래프 시각화 코드 (다음 방법에서 이하 코드 생략)
plt.subplot(121)
classification_result2(X_imb, y_imb)
plt.subplot(122)
model_samp = classification_result2(X_samp, y_samp)

# report출력 코드 
print(classification_report(y_imb, model_samp.predict(X_imb)))
"""
              precision    recall  f1-score   support

         0.0       0.97      0.97      0.97       200
         1.0       0.70      0.70      0.70        20

    accuracy                           0.95       220
   macro avg       0.83      0.83      0.83       220
weighted avg       0.95      0.95      0.95       220
"""
```
- 보고서(classification_report)를 보면 recall이 증가함을 확인할 수 있다.  
- 그래프의 기준선을 보면 왼쪽으로 밀려난 것을 확인할 수 있다.

![](/assets/images/Supervised14_3.png)

### Tomek’s link method
클래스가 다른 두 데이터가 아주 가까이 붙어있으면 토맥링크가 된다. 토맥링크 방법은 이러한 토맥링크를 찾은 다음 그 중에서 **다수 클래스에 속하는 데이터를 제외하는 방법**으로 **경계선을 다수 클래스쪽으로 밀어붙이는 효과**가 있다.

- 즉, **내부에 침투**되어 있거나, **경계선**에 있어 **기준으로서의 가치가 낮은 애들**이 사라진다.

```py
# 샘플링 수행
X_samp, y_samp = TomekLinks(random_state=0).fit_sample(X_imb, y_imb)

# 그래프 시각화 및 리포트 출력 코드 생략
```
![](/assets/images/Supervised14_4.png)

### Condensed Nearest Neighbour
1-NN 모형으로 분류되지 않는 데이터만 남기는 방법이다. 선텍된 데이터 집합을
$S$라고 한다. 
  1. 소수 클래스 데이터를 모두 $S$에 포함시킨다.
  2. 다수 데이터 중에서 하나를 골라서 가장 가까운 데이터가 다수 클래스이면 포함시키지 않고 아니면 $S$에 포함시킨다.
  3. 더이상 선택되는 데이터가 없을 때까지 2를 반복한다.

- Tomek방법과는 다르게 겨계선의 값만 남는다. 따라서 경계선의 information만 남기고 경계선에 해당하지 않는 것들은 버린다. **서포트 벡터 머신**과 원리가 비슷하다.

```py
# 샘플링 수행
X_samp, y_samp = CondensedNearestNeighbour(random_state=0).fit_sample(X_imb, y_imb)

# 그래프 시각화 및 리포트 출력 코드 생략
```
![](/assets/images/Supervised14_5.png)

###  One Sided Selection
토맥링크 방법과 CNN방법을 섞은 것이다. 토맥링크 중 다수클래스를 제외하고 나머지 데이터 중에서도 서로 붙어있는 다수 클래스 데이터는 1-NN 방법으로 제외한다.

```py
# 샘플링 수행
X_samp, y_samp = OneSidedSelection(random_state=0).fit_sample(X_imb, y_imb)

# 그래프 시각화 및 리포트 출력 코드 생략
```
![](/assets/images/Supervised14_6.png)

### Edited Nearest Neighnours
ENN(Edited Nearest Neighbours) 방법은 다수 클래스 데이터 중 가장 가까운 k(`n_neighbors` )개의 데이터 '모두'( `kind_sel="all"` ) 또는 '다수'( `kind_sel="mode"` )가 다수 클래스가 아니면 삭제하는 방법이다. 소수 클래스 주변의 다수 클래스 데이터는 사라진다.

```py
# 샘플링 수행
X_samp, y_samp = EditedNearestNeighbours(kind_sel="all", n_neighbors=5, random_state=0).fit_sample(X_imb, y_imb)

# 그래프 시각화 및 리포트 출력 코드 생략
```
![](/assets/images/Supervised14_7.png)

### Neighbourhood Cleaning Rule
CNN(Condensed Nearest Neighbour) 방법과 ENN(Edited Nearest Neighbours) 방법을 섞은 것이다.

```py
# 샘플링 수행
X_samp, y_samp = NeighbourhoodCleaningRule(kind_sel="all", n_neighbors=5, random_state=0).fit_sample(X_imb, y_imb)

# 그래프 시각화 및 리포트 출력 코드 생략
```

![](/assets/images/Supervised14_8.png)

### 정리
- 경계선을 기준으로 경계선에 있는 것을 없애든, 경계선에서 먼 것을 없애든, 소수의 데이터로 넘어간 애들(다수의 데이터들)을 없애든 해서 데이터의 개수를 줄이는 방법이다. 
- 하지만 샘플 데이터로 **recall값**이 높다고 해서 꼭 해당 모델이 좋은 모델이라고 할 수는 없다.

## 2. 오버 샘플링
소수의 데이터를 늘리는 방법


- RandomOverSampler
- ADASYN
- SMOTE 

```py
from imblearn.over_sampling import *
```

### Random Over Sampling
소수 클래스의 데이터를 반복해서 넣는 것(replacement)이다. 가중치를 증가시키는 것과 비슷하다. **x축 y축이 동일한 데이터**를 랜덤으로 선택해서 반복해 집어 넣는 것이다. (같은 위치에 찍힌다.)

```py
# 위 샘플 데이터 사용
X_samp, y_samp = RandomOverSampler(random_state=0).fit_sample(X_imb, y_imb)
# 그래프 시각화 및 리포트 출력함수 생략(위와 동일)
```
![](/assets/images/Supervised14_9.png)

### ADASYN
ADASYN(Adaptive Synthetic Sampling) 방법은 **소수 클래스 데이터**와 그 데이터에서 **가장 가까운 k개의 소수 클래스 데이터** 중 무작위로 선택된 데이터 사이의 직선상에 가상의 소수 클래스 데이터를 만드는 방법

```py
# 위 샘플 데이터 사용
X_samp, y_samp = ADASYN(random_state=0).fit_sample(X_imb, y_imb)
# 그래프 시각화 및 리포트 출력함수 생략(위와 동일)
```
![](/assets/images/Supervised14_10.png)

### SMOTE
SMOTE (Synthetic Minority Over-sampling Technique) 방법도 ADASYN 방법처럼 데이터를 생성하지만 **생성된 데이터**를 무조건 소수 클래스라고 하지 않고 **분류 모형에 따라 분류**한다.

```py
# 위 샘플 데이터 사용
X_samp, y_samp = SMOTE(random_state=4).fit_sample(X_imb, y_imb)
# 그래프 시각화 및 리포트 출력함수 생략(위와 동일)
```
![](/assets/images/Supervised14_11.png)

## 3. (참고) 복합샘플링 
언더 샘플링 방법과 오버 샘플링 방법을 같이 하는 방법
 
- SMOTEENN : SMOTE + ENN
- SMOTETomek : SMOTE + Tomek 