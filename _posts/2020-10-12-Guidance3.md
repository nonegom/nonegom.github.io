---
title:  "[머신러닝]지도학습 - 1.3. 분류 모형"
excerpt: "분류모형에 관한 개요 확률적 생성 모형/ 확률적 판별 모형 / 판별 함수 모형"

categories:
  - MachinLearning
tags:
  - Guidance&Studying
  - 10월
toc: true
toc_sticky: true
toc_label: 페이지 목차
use_math: true
---
> 아래 포스팅은 기존 수업의 복습 차원에서 올리는 포스팅입니다. 좀 더 구체적인 자료를 원하시면 [데이터 사이언스 스쿨 사이트](https://datascienceschool.net/03%20machine%20learning/09.03%20%EB%B6%84%EB%A5%98%EB%AA%A8%ED%98%95.html) 참고 부탁드립니다. 특히 아래 코드의 plt 그래프의 모습을 보고 싶으시면 확인부탁드립니다.

## 0. 분류모형

- 분류(classification)문제는 독립변수 값이 주어졌을 때 그 값과 가장 연관성이 큰 종속변수값(클래스)을 예측하는 문제이다.
- O,X문제나 객관식 시험 문제를 푸는 것과 비슷하다. (카테고리나 클래스가 미리 주어지기 때문)

- 분류모형은 분류문제를 푸는 방법 (모형은 방법론을 의미한다.)

### 분류모형의 종류

- **확률적 모형**
    - 주어진 데이터($x$)에 대해 각 카테고리 값(혹은 클래스) y들이 정답일 **조건부 확률값**을 구하는 모형
        - 확률적 판별 모형: 베이즈 정리 미사용. 직접 조건부확률 함수의 모양 추정
        - 확률적 생성 모형: 베이즈 정리 사용. 간접적으로 조건부확률을 구함

- **판별함수 모형**
    - 주어진 데이터($x$)의 값이 카테고리에 따라 서로 다른 영역으로 나누는 경계면을 찾아낸 다음, 이 경계면으로부터 주어진 데이터가 어느 위치에 있는지를 계산하는 판별함수를 이용하는 모형

        | 모형       | 방법론      |
        |:----------:|:-----------:|
        | LDA / QDA | 확률적 생성모형 |
        | 나이브 베이지안 | 확률적 생성모형 |
        |로지스틱 회귀| 확률적 판별모형|
        |의사결정나무| 확률적 판별모형|
        |퍼셉트론| 판별함수 모형|
        |서포트벡터머신| 판별함수 모형|
        |인공신경망| 판별함수 모형|

## 1. 확률적 모형
출력변수 y가 K개의 클래스 1, ..., K중의 하나의 값을 가질 경우, 확률적 모형은 카테고리 값의 확률을 모두 구한다. 1000개일 경우 1000번의 확률을 구한다. 그리고 다음의 순서로 x에 대한 클래스 예측을 수행한다.

1. 입력 x가 주어졌을 때 y가 클래스 k가 될 확률 P(y = k l x)을 모두 계산
$$ P_1 = P(y=1|x) \cdots P_K = P(y=K|x) $$

2. 이 중에서 가장 확률이 큰 클래스를 선택하는 방법  
$$ \hat{y} = arg\ \underset{k}maxP(y=k|x)$$

사이킷런 패키지에서 조건부확률 $P(y=k|x)$을 사용하는 분류모형들은 모두 `predict_proba` 메서드와 `predict_log_proba`메서드를 지원한다. 
- `predict_proba`: 독립변수 X가 주어지면, 종속변수 y의 모든 카테고리값을 구해서 확률을 구함.
- `predict_log_proba`:  `predict_proba`값에 log를 취한 확률이 나온다.
  - 로그를 취하는 이유는 확률값이 너무 작아지면 `언더플로우`현상이 발생할 수 있기 때문이다. 그리고 나오는 값이 '양수'이기 때문에 순서는 바뀌지 않는다.

조건부 확률을 계산하는 방법은 두 가지가 있다.
1. 생성모형 (generative model)
2. 판별모형 (discriminative model)

아래에서 각 모형에 대한 설명을 이어가도록 하겠다. 

## 2. 확률적 생성 모형
 생성모형은 각 클래스 별 특징 데이터의 **확률분포** $P(x \ y =k)$을 추정한 다음 **베이즈 정리**를 사용해 $P(y =k \ x)$를 계산하는 방법이다. 위 확률 분포를 '가능도(likelihood)'라고 한다. 

- 베이즈 정리
$$P(y=k|x) = {P(x|y=k)P(y=k)\over P(x)}$$

생성모형에서는 **전체확률의 법칙**을 이용해 특징 데이터 x의 무조건부 확률분포 $P(x)$를 구할 수 있다.

- 전체확률의 법칙
$$ P(x) = \sum_{k=1}^K P(x|y=k)P(y=k)$$

따라서 **확률분포**만 알고 있어도 모형을 만들 수 있고, 이 모형을 통해 가상의 특징 데이터를 만들어낼 수 있다. 예를 들어 만약 입력으로 '사진'데이터가 들어갔을 때, 출력이 '개'일 경우 아래와 같이 식을 구할 수있다.

$$P(y=개|사진) = {P(x|y=개)P(y=개)\over P(사진)}$$

따라서 생성모형을 이용하면, 개나 고양이에 대한 정확한 지식이 있을 경우 개나 고양이를 그릴 수 있게된다는 것이다. 정리하자면, 생성모형은 y값을 알 경우 데이터를 '생성(Generate)'할 수 있기에, **'생성모형'**인 것이다.  
하지만 생성모형은 (가능도를 구하기 위해) 사용하지도 않을 과도한 데이터를 요구하기도 한다. 그렇기에 확률분포를 계산하는데 계산량을 너무 많이 필요로 한다는 단점이 있다.

> 이후 나올 **확률적 생성 모형**에 대한 좀 더 자세한 설명과 코드 사용법은 이후 포스팅에서 별도로 다루도록 하겠다

### 1) QDA(Quadratic Discriminant Analysis)
 QDA는 **조건부확률 기반 생성(generative)모형**의 하나이다.

 ```python
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
model = QuadraticDiscriminantAnalysis().fit(X, y)
 ```

### 2) 나이브 베이지안 모형(Naive Bayesian)
 나이브 베이지안 모형도 조건부 확률 모형의 일종이다. **조건부확률 기반 생성모형**의 장점 중 하나는 **클래스가 3개 이상**인 경우에도 바로 적용할 수 있다는 점이다. 

 ```python
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
news = fetch_20newsgroups(subset="all")
model = Pipeline([
('vect', TfidfVectorizer(stop_words="english")),
('nb', MultinomialNB()),
])
model.fit(news.data, news.target)
 ```

 위의 코드에서는 사이킷런의 `TfidVectorizer` **전처리기**와 `MultinomialNB`**모형**을 사용해 `fetch_20newsgroup`분류 문제를 풀기 위해 로드한 코드이다. 

- `TfidVectorizer`: 텍스트 데이터를 정해진 크기의 실수 벡터로 변환
- `MultinomialNB` : **나이브 베이즈 방법**으로 분류문제를 예측
- `Pipeline`메소드 : 두 클래스 객체를 하나의 모형으로 합치는 메소드. Pipeline으로 합쳐진 모형은 일반적인 모형처럼 fit, predict 등의 메소드를 제공하며 내부의 전처리기 메소드를 상황에 따라 적절히 호출한다.


## 3. 확률적 판별 모형
 **확률적 판별 모형**은 가능도 $p(x | y)$를 구하고 베이즈 정리를 사용해 조건부 확률을 구하던, 확률적 생성 모형과 달리, **조건부 확률 $p(y=1 |x)$이 x에 대한 함수 $f(x)$로 표시될 수 있다고 가정**해, 그 함수를 직접 찾아내는 방법이다. 
$$p(y=k|x)=f(x)$$

- 이 함수 $f(x)$는 확률값이므로 **0보다 같거나 크고, 1보다 같거나 작다**는 조건을 만족해야 한다.

> 이후 나올 **확률적 판별 모형**에 대한 좀 더 자세한 설명과 코드 사용법도 이후 포스팅에서 별도로 다루도록 하겠다

### 1) 로지스틱 회귀모형
 확률적 판별 모형에서 함수 $f(x)$의 모형이 '로지스틱 함수'이면 '로지스틱 회귀 모형'이 된다. .

 ```python
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
# 가상 분류모델 생성
X0, y = make_classification(n_features=1, n_redundant=0,
  n_informative=1, n_clusters_per_class=1, random_state=4)
model = LogisticRegression().fit(X0, y)
 ```

## 4. 판별함수 모형
 또 다른 분류방법 중 하나로 동일한 클래스가 모여 있는 영역이 있을 경우 그 영역을 나누는 경계면을 정의하는 것이다. 이 경계면은 경계면으로부터 거리를 계산하는 $f(x)$ 형태의 함수인 **판별함수(discriminant function)**로 정의된다.

 $$판별 경계선: f(x) = 0$$
 $$클래스 1: f(x) > 0$$
 $$클래스 0: f(x) < 0$$

- 이 모형은 판별 경계선을 기준으로 +인지 -인지의 구분만을 통해서만 클래스를 분류한다.

- `decision_function`메서드 제공
  - 만약, `model.decision_function(x) = y`에서 y의 값이 '- 0.8' 이면 class0이겠구나, y의 값이 '2'이면  class1이겠구나 생각하는 형식이라는 뜻이다.

**단, 판별모형은 클래스가 3개 이상인 문제는 풀지 못한다.**

> 이후 나올 **판별함수 모형**에 대한 좀 더 자세한 설명과 코드 사용법도 이후 포스팅에서 별도로 다루도록 하겠다

### 1) 퍼셉트론(Perceptron)
 가장 단순한 판별함수 모형이다. **직선**이 **경계선(boundart line)**으로 데이터 영역을 나눈다.

 - 만약 데이터의 차원이 3차원이라면 **경계면(boundary surface)**을 가진다. 이러한 '경계선'이나 '경계면'을 **하이퍼 플레인(decision hyperplane)**즉, '고차원 면'이라고 한다. 

```py
from sklearn.linear_model import Perceptron
from sklearn.datasets import load_iris
# iris 데이터 생성
iris = load_iris()
idx = np.in1d(iris.target, [0, 2])
X = iris.data[idx, 0:2]
y = iris.target[idx]
model = Perceptron(max_iter=100, eta0=0.1, random_state=1).fit(X, y)
```

### 2) 커널 SVM (Kernel Support Vector Machine)
 커널 서포트 벡터머신을 사용하면 복잡한 형태의 경계선을 생성할 수도 있다.

```py
from sklearn import svm
# 임의의 데이터 생성
xx, yy = np.meshgrid(np.linspace(-3, 3, 500), np.linspace(-3, 3, 500))
np.random.seed(0)
X = np.random.randn(300, 2)
Y = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)

model = svm.NuSVC().fit(X, Y)
# decision function 메서드 이용
Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])
```

### 3) 다중 클래스 분류
확률적 모형은 클래스가 3개 이상인 경우를 다중 클래스 분류문제도 풀 수 있지만, **판별함수 모형**은 종속변수의 클래스가 2개인 경우를 이진(Binary Class)분류문제 밖에 풀지 못한다.
- 하지만 **OvO(One-Vs-One)**방법이나 **OvR(One-vs-the-Rest)**방법 등을 이용해 여러 개의 이진 클래스 분류 문제로 변환해 푼다.

#### OvO 방법
 $K$개의 클래스가 존재하는 경우, 2개의 클래스 조합을 선택해서  $K(K-1)/2$개의 이진 클래스 분류문제를 풀고, 이진 분류문제를 풀어 가장 많은 결과가 나온 클래스를 선택하는 방법

#### OVR 방법
 **OvO방법**은 클래스의 수가 많아지면 실행해야 할 문제의 수가 너무 많아져, 시간이 길어지게 된다 ($O(K^2)$시간 소요).  
 따라서 **OvR방법**은 각각의 클래스에 대해 표본이 속하는지(y=1) 속하지 않는지(y=0)의 이진 분류 문제를 푼다. 클래스의 수만큼만 이진분류 문제를 풀면된다 ($O(K)$시간만 소요).

 - **OvO방법**과 **OvR방법** 모두 각 클래스가 얻은 조건부 확률값을 모두 더한 값을 비교하여 가장 큰 조건부 확률 총합을 가진 클래스를 선택한다.