---
title:  "[머신러닝] 지도학습 - 6.1. 모형 최적화"
excerpt: "머신 러닝 모형이 완성됐을 경우 예측 성능을 향상시키기 위한 최적화 방법"

categories:
  - ML_Supervised
tags:
  - SupervisedLearning
  - 10월
toc: true
toc_sticky: true
toc_label: 페이지 목차
use_math: true
---
> 아래 포스팅은 기존 수업의 복습 차원에서 올리는 포스팅입니다. 따라서 세부적인 수학적 정리가 생략된 부분이 있습니다. 따라서 좀 더 구체적인 정보나 원하시면 [데이터 사이언스 스쿨 사이트](https://datascienceschool.net/03%20machine%20learning/14.01%20%EB%AA%A8%ED%98%95%20%EC%B5%9C%EC%A0%81%ED%99%94.html)를 참고 부탁드립니다. 특히 아래 코드를 이용한 시각화 그래프 코드와 모습을 보고 싶으시면 링크 확인부탁드립니다.  

## 실무에서 분류모형을 사용할 때 주의해야 할 사항들

- 모형 최적화 (HyperParameter Tuning)
- 비대칭 데이터 문제
- 데이터가 너무 큰 경우(데이터의 종류가 너무 많은 경우, 데이터의 개수가 너무 많은 경우) - 특징 선택과 대규모 데이터 학습

## 0. 모형 최적화
머신 러닝 모형이 완성된 후 최적화 과정을 통해 예측 성능을 향상 시킨다.

### sklearn의 모형 하이퍼 파라미터 튜닝도구

- `validation_curve` : 단일 하이퍼 파라미터 최적화
- `GridSearchCV `: 그리드를 사용한 복수 하이퍼 파라미터 최적화
- `ParameterGrid` : 복수 파라미터 최적화용 그리드

## 1. `validation_curve`
최적화할 파라미터 이름과 범위, 그리고 성능 기준 `param_name`, `param_range`, `scoring` 인수로 받아 파라미터 범위의 모든 경우에 대해 **성능 기준**을 계산한다.

- `param_range`는 logscale로 훑어봐야 한다. $10^{-6} \sim 10^{6}$의 범위라도 logscale을 통해 하면 더 빠르고 쉽게 분석할 수 있다.

```py
from sklearn.datasets import load_digits
from sklearn.svm import SVC
from sklearn.model_selection import validation_curve

digits = load_digits()
X, y = digits.data, digits.target
param_range = np.logspace(-6, -1, 10) 
# 10의 -6승 부터 10의 -1승까지 점을 10개로 나눠서 훑어본다.

%%time
train_scores, test_scores = \
    validation_curve(SVC(), X, y, # 가우시안 커널 사용, gamma값을 추정
                     param_name="gamma", param_range=param_range,
                     cv=10, scoring="accuracy", n_jobs=1) 
                     # cv =10이면, k폴드값이 10
```
- k폴드가 10번은 logspace가 10이므로, 한 포인트에서 k폴드 10번을 하면 총 100번을 한다. 총 100번을 하는데 1분밖에 안 걸린다.
![](/assets/images/Supervised13_1.png)
> 그래프 그리는 코드 생략(상단 출처 참조) 

## 2. `GridSearchCV`
모형이 QDA의 경우 인수가 하나지만, RandomForest같은 경우는 인수가 많아진다. 그래서 `GridSearchCV`을 사용해서 다차원으로 해야할 필요가 있다.  
`validation_curve`와 달리 모형 래퍼(Wrapper)성격의 클래스이다. 클래스 객체에 `fit`메서드를 호출하면 grid search를 사용해 자동으로 복수 개의 내부 모형을 생성하고 이를 모두 실행시켜 최적의 파라미터를 찾아준다. 생성된 복수 개의 내부 모형과 실행 결과는 다음 속성에 저장된다.


- `grid_scores_`
    - `param_grid` 의 모든 파리미터 조합에 대한 성능 결과. 각각의 원소는 다음 요소로 이루어진 튜플이다.
    - `parameters`: 사용된 파라미터
    - `mean_validation_score`: 교차 검증(cross-validation) 결과의 평균값
    - `cv_validation_scores`: 모든 교차 검증(cross-validation) 결과
- `best_score_`: 최고 점수
- `best_params_`: 최고 점수를 낸 파라미터
- `best_estimator_`:최고 점수를 낸 파라미터를 가진 모형

```py
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC

# StandScalar라는 전처리기 사용, SVC 모델 사용
pipe_svc = Pipeline([('scl', StandardScaler()), ('clf', SVC(random_state=1))])
param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0] 
# -0.0001 ~ 1000.0 까지 8번

## SVC에서 두 개의 커널 'linear'와 'rbf'를 사용할 수 있다.
param_grid = [
    {'clf__C': param_range, 'clf__kernel': ['linear']}, # 8번 
    {'clf__C': param_range, 'clf__gamma': param_range, 'clf__kernel': ['rbf']}] # 8x8 = 64번

## 72번 피팅을 한다. 그리고 cv를 통해 10번을 한다. 총 720번의 search를 진행한다.
gs = GridSearchCV(estimator=pipe_svc, param_grid=param_grid,
                  scoring='accuracy', cv=10, n_jobs=1)

%time gs = gs.fit(X, y)
# fitting에 7분 정도의 시간이 걸린다.

print(gs.best_score_)
print(gs.best_params_)
```

## 3. ParameterGrid
'분산처리'를 위해 일을 나눠줄 때, 자동적으로 데이터를 나눠주는 메소드가 `ParameterGrid`이다. `ParameterGrid`는 탐색을 위한 iterator의 역할을 한다. `GridSearchCV`이외의 방법으로 그리드 탐색을 해야하는 경우에 사용

```py
from sklearn.model_selection import ParameterGrid
# a케이스
param_grid = {'a': [1, 2], 'b': [True, False]}
list(ParameterGrid(param_grid))
""" >
[{'a': 1, 'b': True},
{'a': 1, 'b': False},
{'a': 2, 'b': True},
{'a': 2, 'b': False}]
"""

# b케이스
param_grid = [{'kernel': ['linear']}, {'kernel': ['rbf'], 'gamma': [1, 10]}]
list(ParameterGrid(param_grid))
""">
[{'kernel': 'linear'},
{'gamma': 1, 'kernel': 'rbf'},
{'gamma': 10, 'kernel': 'rbf'}]
"""
```

## 4. 병렬처리
위의 방법을 조금 더 빠르게 하고 싶다면, 병렬적으로 동시에 여러 작업을  돌리게 하면 된다. `GridSearchCV`메소드에서 `n_jobs`라는 인수를 통해 병렬처리할 수 있는데, 내부적으로 CPU의 멀티 프로세스를 사용해 그리드 서치를 수행한다. `n_jobs`를 늘릴수록 속도가 증가하지만, CPU코어의 갯수만큼만 증가시키는게 의미가 있다.

### 참고) 분산처리
만약 CPU의 코어 수가 부족하다면, 하드웨어를 여러 개 연결해서 일을 분산해서 처리할 수 있다. 하지만 그러기 위해서는 DASC라는 프로그램을 사용할 수 있는데, 그러기 위해서는 사용환경을 맞춰야 하므로 '인프라 프로그래밍'을 활용해야 한다.

```py
from sklearn.model_selection import GridSearchCV
param_grid = {"gamma": np.logspace(-6, -1, 6)}
gs1 = GridSearchCV(estimator=SVC(), param_grid=param_grid,
                   scoring='accuracy', cv=5, n_jobs=1)
gs2 = GridSearchCV(estimator=SVC(), param_grid=param_grid,
                   scoring='accuracy', cv=5, n_jobs=2) # n_jobs = 2

%%time
gs1.fit(X, y)
# > Wall time: 11.1s ...

%%time
gs2.fit(X, y)
# > Wall timt: 7.44 s ...
# 시간이 2배 정도 단축된다.

```