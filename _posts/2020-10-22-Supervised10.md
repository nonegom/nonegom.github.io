---
title:  "[머신러닝] 지도학습 - 5.1. 추천시스템"
excerpt: "surprise 패키지를 활용한 추천시스템 알고리즘 및 코드"

categories:
  - MachinLearning
tags:
  - SupervisedLearning
  - 10월
toc: true
toc_sticky: true
toc_label: 페이지 목차
use_math: true
---
> 아래 포스팅은 기존 수업의 복습 차원에서 올리는 포스팅입니다. 따라서 세부적인 수학적 정리가 생략된 부분이 있습니다. 따라서 좀 더 구체적인 정보나 원하시면 [데이터 사이언스 스쿨 사이트](https://datascienceschool.net/03%20machine%20learning/07.01%20%EC%B6%94%EC%B2%9C%20%EC%8B%9C%EC%8A%A4%ED%85%9C.html)를 참고 부탁드립니다. 특히 아래 코드를 이용한 시각화 그래프 코드와 모습을 보고 싶으시면 링크 확인부탁드립니다.  

## 0. 추천 시스템
추천 시스템(recommender system)이란 사용자(user)가 선호하는 상품(item)을 예측하는 시스템이다. 아마존이나 넷플릭스 등의 사이트는 사용자가 각각의 상품에 대해 평가한 평점(rate)을 가지고 있다. 이 기록을 기반으로 해서 사용자에게 어떤 상품을 추천할지 에측한다.
- 무료 라이브러리인 `Surprise`패키지를 사용해 진행

## 1. Surprise 패키지
`Surpris`패키지는 다양한 추천 시스템 알고리즘을 제공한다. 그 중 `MovieLense`라는 영화 추천 웹사이트의 데이터를 샘플 평점 데이터로 제공한다. 그 중 10만개의 샘플 데이터 세트를 로드해서 데이터 프레임으로 변환한다. 
```py
import surprise

# 0. 데이터 로드
data = surprise.Dataset.load_builtin('ml-100k')

# 1. 데이터 프레임 변환 ("id 열 제거")
df = pd.DataFrame(data.raw_ratings, columns = ["user", "item", "rate", "id"])
del df["id"]
```

|user|item|rate|
|:----:|:----:|:----:|
|사용자 아이디|상품 아이디|평점|

- 추천 시스템은 사용자 아이디와 상품 아이디라는 두 개의 **카테고리 입력**과 **평점 출력**을 가지는 예측 시스템이다. 

위 데이터를 피벗테이블 형태로 만들면 x축이 상품, y축이 사용자 아이디인 평점 행렬 $R$이 된다. 평점 행렬 $R$의 행은 특정 사용자의 평점이고 평점 행렬 $R$의 열은 특정 상품의 평점이다.

- 이 행렬을 살펴보면 평점 데이터가 일부 위치에만 존재하는 sparse행렬임을 알 수 있다. 

```py
# 평점 행렬 생성
df_table = df.set_index(["user", "item"]).unstack()
df_table.shape

# 평점 행렬의 빈칸을 흰색, 점수를 검은색으로 시각화
plt.imshow(df_table)
plt.grid(False)
plt.xlabel("item")
plt.ylabel("user")
plt.title("Rate Matrix")
plt.show()
```
![](/assets/images/Supervised10_1.JPG)

### 추천 시스템 알고리즘
추천 시스템은 **두 개의 카테고리 값 입력**에서 하나의 실수 값 출력을 예측하는 회귀모형이지만 여러가지 방법으로 예측 성능을 향상시키고 있다. 
1. 베이스라인 모형
  - 데이터의 개수가 너무 많아 'Numerical Optimization' 이용
2. Collaborative Filtering: 회귀분석이 아닌 다른 방법(주로 설명할 방법)
  - 2.1. Neighborhood Models
    - User-based CF
    - Item-based CF
  - 2.2.Latent Factor Models
    - Matrix Factorization
    - SVD
3. Content-Based Recommandion

## 1. 베이스라인 모형
베이스라인 모형(baseline model)은 사용자 아이디 $\mu$, 상품 아이디 $i$, 두 개의 카테고리 값 입력에서 평점 $r(u, i)$의 예측치 $\hat{r}(u ,i)$을 예측하는 가장 단순한 회귀분석 모형이다. **사용자와 상품 특성에 의한 평균평점의 합**으로 나타난다.
$$\hat{r}(u,i) = \mu + b(u) + b(i)$$
이 식에서 $\mu$는 전체 평점의 평균이고, $b(u)$는 동일한 사용자에 의한 평점 조정값 (특정 유저가 주는 평점), $b(i)$는 동일한 상품에 대한 평점(아이템의 기본값)조정값이다.  
베이스 라인 모형은 아래와 같은 오차함수를 최소화하도록 구해진다. 여기서 $R_{trian}$은 **실제 평점이 존재하는 학습용 데이터 집합**이다. 과최적화를 피하기 위해 정규화 항을 추가할 수도 있다.
$$\sum_{u, i \in R_{trian}} \left( r(u,i)-\hat{r}(u, i) \right)^2 $$ 

- 데이터가 너무 많아져서 쓸데없는 행렬도 너무 만들게 된다는 단점도 존재한다.

### 알고리즘
surprise 패키지는 오차 함수를 최소화하기 위해 두 가지 최적화 알고리즘을 제공한다. 알고리즘의 선택은 `method` 인수를 사용한다.
- SGD(Stochastic Gradient Descent)의 인수
  -  reg : 정규화 가중치. 디폴트는 0.02.
  -  learning_rate : 최적화 스텝 사이즈. 디폴트는 0.005.
  -  n_epochs : 최적화 반복 횟수. 디폴트는 20.

- ALS(ALternating Least Squares)의 인수 
  -  reg_i : 상품에 대한 정규화 가중치. 디폴트는 10.
  -  reg_u : 사용자에 대한 정규화 가중치. 디폴트는 15.
  -  n_epochs : 최적화 반복 횟수. 디폴트는 10.

### 모형 사용법
베이스 라인 모형을 비롯한 surprise 패키지 모형을 사용하기 위해 다음과 같은 순서를 거친다.
1. 데이터 세트의 `split` , `folds` 메소드를 사용해서 **K-Fold 트레이닝 데이터셋**과 **테스트 데이터셋**을 만든다.
2. **모형 알고리즘 객체를 생성**한다.
3. 모형 알고리즘 객체의 **`train` 메서드**와 트레이닝 데이터셋으로 **모수를 추정**한 후, **`test 메서드`**로 테스트 데이터셋에 대한 예측을 실시한다.
4. `accuracy` 서브패키지의 **성능평가 함수를 사용하여 예측 성능을 계산**한다.

- 위 과정은 `evaluate` 명령으로 단축할 수도 있다.


surprise 패키지는 베이스라인 모형을 위한 `BaselineOnly`클래스를 제공한다. 

### 추천 성능 평가 기준
`accuracy` 서브패키지에서는 추천성능 평가기준을 제공한다. 
- RMSE (Root Mean Squared Error)
- MAE (Mean Absolute Error)
- FCP (Fraction of Concordant pairs)

```py

## 1. Baseline 모형 생성 (앞선 코드에서 생성한 'data'변수 사용)
from surprise.model_selection import KFold
bsl_options = {
    'method': 'als',
    'n_epochs': 5,
    'reg_u': 12,
    'reg_i': 5
}
algo = surprise.BaselineOnly(bsl_options)

## 2. 정석적인 풀이
np.random.seed(0)
acc = np.zeros(3)
cv = KFold(3)
for i, (trainset, testset) in enumerate(cv.split(data)):
    algo.fit(trainset)
    predictions = algo.test(testset)
    acc[i] = surprise.accuracy.rmse(predictions, verbose=True)
acc.mean()


## 3. cross_validate 명령 사용
from surprise.model_selection import cross_validate
cross_validate(algo, data)

```
위에서 설명한 방식대로 모형을 사용할 수도 있지만, 'cross_validate'를 이용해서 한번에 성능평가를 할 수 있다.

## 2. Collaborative Filter
Collaborative Filter(CF)방법은 모든 사용자의 데이터를 균일하게 사용하는 것이 아니라 **평점 행렬이 가진 특정한 패턴**을 찾아서 이를 평점 예측에 사용하는 방법이다. Neighborhood 모형과 Latent Factor 모형이 있다.

### 1) Neighborhood 모형
Memory-based CF라고도 한다. 특정 사용자의 평점을 예측하기 위해 사용하는 것이 아니라 **해당 사용자와 유사한(similar) 사용자에 대해 가중치**를 준다.  Neighborhood모형은 방법에 따라 두 경우로 나뉜다.


- 사용자 기반(User-based) CF: 해당 사용자와 유사한 사용자를 찾는 방법, 평점 행렬에서 유사한 사용자 행 벡터를 찾아 이를 기반으로 빈 데이터를 계산한다.
- 상품 기반(Item-based) CF: 특정한 상품에 대해 사용자가 준 점수 즉, 평점 행렬에서 상품 열 벡터의 유사성을 찾고 특정 상품과 유사한 평점 정보를 가지는 상품들로 해당 상품의 빈 데이터를 예측하는 방법

#### - 유사도 계산
사용자 특성 벡터(평점 행렬의 행 벡터)나 상품 특성 벡터(평점 행렬의 열 벡터)의 유사도(similarity)를 비교하기 위한 여러 기준을 surprise 패키지에서도 제공한다.
- 평균제곱차이 유사도 (MSD, Mean Squared Difference Similarity)
- 코사인 유사도 (Cosine Similarity)
- 피어슨 유사도 (Pearson Similarity): 두 벡터의 상관계수
- 피어슨-베이스라인 유사도 (Pearson-Baseline Similarity)

> 각 알고리즘에 대한 세부적인 정의는 생략

surprise 패키지의 유사도 설정 옵션은 다음과 같다.
- `name` : 사용할 유사도의 종류를 나타내는 문자열. 디폴트는 'MSD' .
- `user_based` : True 면 사용자 기반, False 면 상품 기반.
- `min_support` : 두 사용자나, 상품에서 공통적으로 있는 평점 원소의 수의 최솟값. 공통 평점 원소의 수가 이 값보다 적으면 해당 벡터는 사용하지 않는다. 
- `shrinkage` : Shrinkage 가중치. 디폴트는 100.

```py
## 평균제곱차이 유사도
sim_options = {'name': 'msd'}
algo = surprise.KNNBasic(sim_options=sim_options)
cross_validate(algo, data)["test_mae"].mean()
# > 0.7726801901092284

## 코사인 유사도
sim_options = {'name': 'cosine'}
algo = surprise.KNNBasic(sim_options=sim_options)
cross_validate(algo, data)["test_mae"].mean()
# > 0.8046567723959086

## 피어슨 유사도
sim_options = {'name': 'pearson'}
algo = surprise.KNNBasic(sim_options=sim_options)
cross_validate(algo, data)["test_mae"].mean()
# > 0.8032778978216127

## 피어슨-베이스라인 유사도
sim_options = {'name': 'pearson_baseline'}
algo = surprise.KNNBasic(sim_options=sim_options)
cross_validate(algo, data)["test_mae"].mean()
# >0.791677323191221
```

#### - KNN 가중치 예측 방법
유사도가 구해지면 평점을 예측하고자 하는 **사용자(또는 상품)**와 **유사도가 큰** $k$개의 사용자(또는 상품) 벡터를 사용하여 **가중 평균을 구해서 가중치를 예측**한다. 이러한 방법을 KNN(K Nearest Neighbors) 기반 예측 방법이라고 한다.  
surprise 패키지에서는 3가지의 KNN 기반 가중치 예측 알고리즘 클래스를 제공한다.
- KNNBasic: 평점들을 단순히 가중 평균한다.
- KNNWithMeans: 평점들을 평균값 기준으로 가중 평균한다.
- KNNBaseline: 평점들을 베이스라인 모형의 값 기준으로 가중 평균한다.

```py

## KNNBasic
# 앞선 코드에서 사용한 예측 방법이 'KNNBasic'이다.

## KNNWithMeans
sim_options = {'name': 'pearson_baseline'}
algo = surprise.KNNWithMeans(sim_options=sim_options)
cross_validate(algo, data)["test_mae"].mean()
# > 0.7299485648439766

## KNNBaseline
sim_options = {'name': 'pearson_baseline'}
algo = surprise.KNNBaseline(sim_options=sim_options)
cross_validate(algo, data)["test_mae"].mean()
# > 0.7211170375266851

```

### 2) Latent Factor 모형
사용자의 특성 벡터나 상품의 특성 벡터의 길이가 매우 긴 크기가 될 경우, 이 **특성들을 몇 개의 요인 벡터로 간략화 할 수 있다**는 가정에서 출발한 모형이다. PCA를 사용하면 긴 특성 벡터를 소수의 차원으로 차원 축소할 수 있듯이 사용자의 특성도 차원 축소할 수 있다.  
 한 예시로 영화에 대한 평점을 주는 경우, 몇 개의 '장르 요인'이 있어서 사용자는 특정한 장르 요소에 대해 대해 점수를 더 많이 주거나 적게 줄 수 있다. 또한 어떤 영화 자체에도 이러한 '장르 요인'이 있다면 해당 사용자의 영화에 대한 평점은 '사용자의 장르 요인 벡터'와 '영화의 장르 요인 벡터'의 내적으로 표시할 수 있다.

#### - MatrixFactorization
모든 사용자와 상품에 대해 다음 오차 함수를 최소화하는 요인 벡터를 찾아낸다. 즉 다음과 같은 행렬 $P, Q$를 찾는다.
$$R \approx PQ^T$$

여기에서
- $R \in R^{m \times n}$: $m$사용자와 $n$상품의 평점 행렬
- $P \in R^{m \times k}$: $m$사용자와 $k$요인의 관계 행렬
- $Q \in R^{n \times k}$: $n$상품과 $k$요인의 관계 행렬

#### - SVD
SVD(Singular Value Decomposition) 는 Matix Factorization 문제를 푸는 방법 중 하나이다. $m \ \times \ n$ 크기의 행렬 $R$은 다음과 같이 세 행렬의 곱으로 나타낼 수 있다. 이를 **특이치 분해**(Singular Value Decomposition)라고 한다.
$$R = U\Sigma V^T$$

여기에서
- $U$: $m \times m$ 크기의 행렬로 역행렬이 대칭 행렬
- $\Sigma$: $m \times n$ 크기의 행렬로 비대각 성분이 0인 행렬
- $V$: $n \times n$ 크기의 행렬로 역행렬이 대칭 행렬

$\Sigma$의 대각 성분은 **특이치**라고 하며 전체 특이치 중에서 가장 값이 큰 $k$개의 특이치만을 사용(Truncated SVD)해서 다음과 같은 특정한 행렬을 만들어, 원래의 행렬과 같은 크기를 가지고 유사한 원소를 가지는 행렬 $\hat{R}$을 만든다.
- 하지만 실제로 평점 행렬은 빈 원소가 많은 spare 행렬이기 때문에, SVD를 바로 적용하기 힘들다. 따라서 행렬 $P, Q$는 특정한 모형에 대해 오차 함수를 최소화해 구한다.

```py
## SVD
algo = surprise.SVD(n_factors=100)
cross_validate(algo, data)["test_mae"].mean()
# > 0.739210803693828

## NMF(Non-negative matrix factorization)
algo = surprise.NMF(n_factors=100)
cross_validate(algo, data)["test_mae"].mean()
# > 0.8371368390504086
```

## 참고
- 실제적으로 추천 시스템을 구현하기 위해서는 여러가지 고려사항들이 있기 마련이다. 반복적으로 구입가능한 '생필품'이나 '식품류'등의 경우를 추천해야 하는 경우가 있을 수 있다.
- 지금까지 위의 방법론은 기존에 '평점'이라는 항목이 존재했다. 하지만 '평점'이 없다면 Contents Based(컨텐츠 기반)를 통해 상품에 대한 특징을 임의로 분해해서 '유사도'를 정하고, 유저의 속성을 기반으로 추천을 한다.
- 하지만 컨텐츠 기반 추천방법은 그 방법이 옳다는 보장이 없기 때문에 평점이라는 기반이 없을 때 추천을 해야 하는 경우 사용한다. 또한 한 유저의 속성으로만 에측하는 것이 것이 아니라 여러 유저의 속성을 기반으로 예측해서 추칙한다.

