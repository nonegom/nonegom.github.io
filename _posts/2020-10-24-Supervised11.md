---
title:  "[머신러닝] 지도학습 - 5.2. 서포트 벡터 머신"
excerpt: "확률적 판별 모형 중 서포트 벡터 머신에 대한 설명"

categories:
  - MachinLearning
tags:
  - SupervisedLearning
  - 10월
toc: true
toc_sticky: true
toc_label: 페이지 목차
use_math: true
---
> 아래 포스팅은 기존 수업의 복습 차원에서 올리는 포스팅입니다. 따라서 세부적인 수학적 정리가 생략된 부분이 있습니다. 따라서 좀 더 구체적인 정보나 원하시면 [데이터 사이언스 스쿨 사이트](https://datascienceschool.net/03%20machine%20learning/13.02%20%EC%84%9C%ED%8F%AC%ED%8A%B8%20%EB%B2%A1%ED%84%B0%20%EB%A8%B8%EC%8B%A0.html)를 참고 부탁드립니다. 특히 아래 코드를 이용한 시각화 그래프 코드와 모습을 보고 싶으시면 링크 확인부탁드립니다.  

## 0. 서포트 벡터 머신
퍼셉트론은 가장 단순하고 빠른 판별 함수 기반 분류 모형이지만 **판별 경계선**(decision hyperplane)이 유니크하게 존재하지 않는다는 특징이 있다. **서포트 벡터 머신(SVM: support vector machine)**은 퍼셉트론 기반 모형에서 가장 안정적인 판별 경계선을 찾기 위한 제한 조건을 추가한 모형이라고 볼 수 있다.
![](/assets/images/Supervised11_1.JPG)

## 1.  서포트와 마진
### 서포트
판별함수모형에서 y는 +1, -1 두 개의 값을 가진다. $y$값이 +1인 데이터 중에서 판별 함수의 값이 가장 작은 데이터를 $x^+$라고 하고 $y$값이 -1인 데이터 중에서 판별함수의 값이 가장 큰 데이터를 $x^-$라고 하자.이 $x^+$와 $x^-$를 **서포트 벡터(support vector)**라고 한다. **서포트벡터**는 각각의 클래스에 속한 데이터 중에서 가장 경계선에 가까이 붙어있는 최전방(most front)의 데이터들이다.
>  **'서포트 벡터'**원래는 하나이지만, 특수한 경우 여러 개가 될 수 있다.

서포트에 대한 판별함수의 값 $f(x^+), f(x^-)$값은 부호 조건만 지키면 어떤 값이 되어도 괜찮다. 따라서 아래와 같은 조건을 만족하게 판별함수를 구한다.
$$f(x^+) = w^Tx^+ - w_0 = +1$$
$$f(x^-) = w^Tx^- - w_0 = -1$$

이렇게 되면 모든 $x_+, x_-$ 데이터에 대해 판별함수의 값의 절대값이 1보다 커지므로 다음 부등식이 성립한다.
$$w^Tx_+ - w_0  \ge-1$$
$$w^Tx_- - w_0  \le-1$$

### 마진
판별 경계선 $w^Tx - w_0 = 0$과 점 $x^+, x^-$사이의 거리는 **선형대수의 기하학적 원리에서 '선'과 '점' 사이의 거리 공식(직선에 수직인 값[길이])**을 통해 같이 구할 수 있다.
$$\frac{w^Tx^+ -w_0}{||w||} = \frac{1}{||w||}$$
$$ -\frac{w^Tx^- -w_0}{||w||} = \frac{1}{||w||}$$

이 거리의 합을 **'마진(margin)'**이라고 한다.**마진**은 서포트에서 경계선 까지 거리의 합이라고 할 수 있다. 직선의 방정식에서 직선에 수직인 값을 구하는 게 길이가 된다.  **마진 값(거리가 멀 수록)이 클 수록 더 경계선이 안정적**이라고 볼 수 있다. 그런데 위에서 정한 **스케일링**에 의해 마진은 다음과 같이 정의가 가능하다.
$$\frac{w^Tx^+ -w_0}{||w||} -\frac{w^Tx^- -w_0}{||w||} = \frac{2}{||w||}$$

마진 값이 최대가 되는 경우는 $||w||$ 값이 최소가 되는 경우와 같다. 이 경우는 $||w^2||$이 최소가 되는 경우와 같다. 
따라서 다음과 같은 목적함수를 최소화하면 된다.
$$L = \frac{1}{2}||w||^2 = \frac{1}{2}w^Tw$$  
$||w||$값을 제곱하는 이유는 w값을 선형대수식으로 만들 수 있기 때문이다. 

또한 스케일링을 사용하여 모든 데이터에 대해 $f(x) = w^Tx_i - w_0$가 1보다 크거나 -1보다 작게 만들었다는 점을 이용해서, **모든 표본 데이터에 대해 분류**가 제대로 되도록하기 위해 모든 데이터 $x_i,y_i (i = 1, \dots, N)$에 대해 다음 조건을 만족하게 한다.
$$y_i \cdot f(x_i) = y_i \cdot (w^Tx_i - w_0) \ge  1\;  (i = 1, \dots, N)$$
$$y_i \cdot (w^Tx_i - w_0) - 1 \ge  0 \; (i = 1, \dots, N)$$
- 위의 경우는 부등식 제한조건이 있는 경우

위를 **라그랑주 승수법**을 사용하면 **최소화 목적함수**를 다음과 같이 고칠 수 있다.
$$L = \frac{1}{2}w^Tw - \sum^N_{i=1} a_i\{y_i \cdot (w^Tx_i - w_0) - 1\}$$

여기에서 **$a_i$**값은 각각의 부등식에 대한 **라그랑주 승수**이다. 이 최적화 문제를 풀어 $w, w_0, a$를 구하면 **판별함수**를 얻을 수 있다.  


그런데 여기서 KKT 조건(조건 중 2번째 조건)에 따르게 되면, 부등식 제한조건이 있는 경우 i번째 부등식이 있으나 없으나 답이 같은 경우에는 라그랑지 승수의 값은 $a_i =0$이 된다. 이 경우는 **판별 함수**의 값이 -1보다 작거나 큰 경우에는 **'라그랑지 승수'값이 0**이 된다.

$$y_i \cdot (w^Tx_i - w_0) - 1 \ge  0 \; $$

다시 말해 학습 데이터 중 서포트 벡터가 아닌 모든 데이터들에 대해서는 위 조건이 만족됨으로, 서포트 벡터가 아닌 데이터는 **'라그랑지 승수가 0'**이다. 따라서 서포트 벡터만 만족되면 나머지는 자동적으로 만족된다는 뜻이다.

#### - KKT조건
f(x)대신 다른 것을 최적화시킬 때, 제한 조건이 부등식일 경우 다음과 같은 조건을 따른다.
    1. x로 미분한 값이 0이어야 한다.
    2. 람다($\lambda$)로 미분한 값이나 람다($\lambda$)가 0이어야 한다.
    3. 람다($\lambda$)값은 0보다 크거나 같다. 
    
## 2. 듀얼 형식
그런데 이렇게 되면 데이터의 갯수의 2배만큼 문제를 풀어야 하기 때문에, 시간이 오래걸린다. 따라서 좀 더 쉬운 방법인 **'듀얼형식(dual form)'**을 만들게 된다. 
- **최적화 조건**은 목적함수 $L$을 $w, w_0$로 미분한 값이 0이 되어야 한다. (KKT조건 중 1번 조건)
$$\frac {\partial L}{\partial w} = 0, \quad \frac {\partial L}{\partial w_0} = 0$$

위 식을 정리하면, 아래와 같이 나온다. (자세한 정리는 생략)
$$w = \sum^N_{i=1}a_iy_ix_i, \quad 0 = \sum^N_{i=1}a_iy_i$$
 
위 두 수식을 원래의 목적함수에 대입해서 $w, w_0$를 없애면 최종적으로 아래와 같은 식이 된다. (일차형식 - 이차형식 = 이차 형식)
$$L = \sum^N_{i=1} a_I - \frac{1}{2}\sum^N_{i=1}\sum^N_{j=1}a_ia_jy_iy_jx_i^Tx_j$$

이 때, a는 다음 조건을 만족한다.
$$ \sum^N_{i=1} a_iy_i= 0, \quad a_i \ge 0 (i = 1, \dots, N)$$

따라서 이 문제는 $w$를 구하는 문제가 아니라 $a$만을 구하는 문제로 바뀌었으므로 **'듀얼형식(Dual Form)'**이라고 한다. 듀얼형식으로 바꾸는 이유는, 듀얼형식으로 바꾸게 되면 수치적으로 '박스 제한조건'이 있는 **이차프로그래밍(QP, Quadratic Programming)문제**가 된다. 따라서 원래의 문제보다는 효율적으로 풀 수가 있어진다.   
 
 
- 듀얼 형식 문제를 풀어 **함수 $L$**을 최소화하는 **$a$**를 구하면 예측 모형을 다음과 같이 쓸 수 있다. 
$$f(x) = w^Tx -w^0 = \sum_{i=1}^Na_iy_ix_i^Tx - w_0$$

w_0의 값은 아래 셋 중 하나의 식으로 구한다.
$$w_0 = w^Tx^+ -1 $$
$$w_0 = w^Tw^- +1 $$
$$w_0 = \frac{1}{2}w^T(x^+ +x^-)$$

그런데 라그랑주 승수 값이 0 즉 $a_i = 0$이면 해당 데이터는 예측모형 $w$ 계산에 아무런 기여를 하지 않으므로 $f(x)$의 식은 다음과 같아진다.
$$f(x) = a^+x^Tx^+ - a^-x^Tx^- -w_0$$

여기에서 $x^Tx^+$는 **$x$와 $x^+$사이**의 (코사인)유사도, $x^Tx^-$는 **$x$와 $x^-$사이**의 (코사인)유사도이므로 결국 **두 서포트 벡터와의 유사도를 측정해서 값이 큰 쪽으로 (예측모형이) 판별**하게 된다. 

## 3. Scikit-Learn의 서포트 벡터 머신

Scikit-Learn의 svm 서브페키지는 서포트 벡터 머신 모형인 `SVC` (Support Vector Classifier) 클래스를 제공  

`SVC` 클래스는  다음과 같은 인수를 갖는다.
- `kernel`: 커널을 선택하는 인수
- `C` : 슬랙변스 가중치를 선택하는 인수

만약 위에서 공부한 '서포트 벡터 머신'을 사용하려면 인수를 `SVC(kernel = 'linear', C = 1e10)`처럼 넣어준다. (인수에 대한 것은 추후 설명)

```py
## 0. 모델 생성
from sklearn.datasets import make_blobs
X, y = make_blobs(n_samples=50, centers=2, cluster_std=0.5, random_state=4)
y = 2 * y - 1  # y값을 -1과 1로 만들었다.
 
# 시각화 코드
plt.scatter(X[y == -1, 0], X[y == -1, 1], marker='o', label="-1 클래스")
plt.scatter(X[y == +1, 0], X[y == +1, 1], marker='x', label="+1 클래스")
plt.xlabel("x1")
plt.ylabel("x2")
plt.legend()
plt.title("학습용 데이터")
plt.show()

# 1. 모델 생성
from sklearn.svm import SVC
model = SVC(kernel='linear', C=1e10).fit(X, y)
```
![](/assets/images/Supervised11_2.png)

### SVC가 가지는 속성값

- `n_support_` : 각 클래스의 서포트의 개수
- `support_` : 각 클래스의 서포트의 인덱스 (어떤 데이터가 서포트인지)
    - `y[model.supprt_]`를 할 경우 **서포트 벡터의 값**
- `support_vectors_` : 각 클래스의 서포트의 $x$ 값. $x^+$와 $x^-$, 각 서포트 벡터의 위치
- `coef_` : $w$벡터
- `intercept_` : $-w_0$
- `dual_coef_` : 각 원소가 $a_i \cdot y_i$ 로 이루어진 벡터
- `decision_function`: 데이터의 판별함수 값 계산 (테스트 데이터 삽입 시 확인)

```py
# 데이터 처리 (서포트 벡터 분리, 그에 따라 분리되는 값들 구분)
xmin = X[:, 0].min()
xmax = X[:, 0].max()
ymin = X[:, 1].min()
ymax = X[:, 1].max()
xx = np.linspace(xmin, xmax, 10)
yy = np.linspace(ymin, ymax, 10)
X1, X2 = np.meshgrid(xx, yy)

Z = np.empty(X1.shape)
for (i, j), val in np.ndenumerate(X1):
    x1 = val
    x2 = X2[i, j]
    p = model.decision_function([[x1, x2]])
    Z[i, j] = p[0]

levels = [-1, 0, 1]
linestyles = ['dashed', 'solid', 'dashed']
plt.scatter(X[y == -1, 0], X[y == -1, 1], marker='o', label="-1 클래스")
plt.scatter(X[y == +1, 0], X[y == +1, 1], marker='x', label="+1 클래스")
plt.contour(X1, X2, Z, levels, colors='k', linestyles=linestyles)

# 서포트 벡터 그리기
plt.scatter(model.support_vectors_[:, 0], model.support_vectors_[
            :, 1], s=300, alpha=0.3)

# 테스트 데이터 생성
x_new = [10, 2]
plt.scatter(x_new[0], x_new[1], marker='^', s=100)
plt.text(x_new[0] + 0.03, x_new[1] + 0.08, "테스트 데이터")

# 시각화
plt.xlabel("x1")
plt.ylabel("x2")
plt.legend()
plt.title("SVM 예측 결과")

plt.show()
```
![](/assets/images/Supervised11_3.png)


## 4. 슬랙변수

데이터가 선형분리가 불가능한 경우, **슬랙변수(slack variable)** $\xi$를 사용해 개별적인 오차 허용이 가능하다.

- 양수인 슬랙변수 $\xi>= 0$을 사용하면, 원래 판별함수의 각 클래스 영역의 샘플들에 대해 조건을 완화할 수 있다.
$$w^Tx_+ -w_0 \le +1 -\xi_i$$
$$w^Tx_- -w_0 \le -1 +\xi_i$$

이렇게 했을 때, y를 곱하게 되면 결국 아래와 같이 된다.
$$y(w^Tx_+ -w_0) \le -1 + \xi_i$$

이렇게 되면 **모든 슬랙변수는 0보다 같거나 크다.** 만약 파고 들어가지 않은 애들은 슬랙함수($xi$)의 값은 0이다.  


따라서 부등식 조건을 모두 고려한 최적화 목적함수는 다음과 같아진다. **$\xi$만 추가되는 것이 아니라**, 부등식 제한조건이 추가가 되므로 **라그랑주 승수법**을 또 적용시켜야 한다
$$L = \frac{1}{2}||w||^2 = \sum^N_{i=1}a_i(y_i)\cdot (w^Tx_i - w_0) -1 + \xi) - \sum^N_{i=1}\mu_i\xi_i+C\sum^N_{i=1}\xi_i$$

- 여기서 $ C\sum^N_{i=1}\xi_i$ 항은 슬랙변수의 합이 너무 커지지 않도록 **제한하는 역할**을 한다. 따라서 C가 커지면, 슬랙변수가 작아지게 만든다. 따라서 C를 조절함에 따라 마진이 작아지거나 커지게 된다. 
- 즉, **C가 커지면** 슬랙변수를 만들지 않으려고 **'마진이 작아진다'**
- 반대로, **C가 작아지면** 슬랙변수를 만들어도 상관이 없으니 **'마진이 커진다'**

```py
## 0. 샘플 데이터 생성
np.random.seed(0)
X = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]
Y = [-1] * 20 + [1] * 20

## 1. C값과 패널티 값에 변화를 준 그래프 출력
for name, penalty in (('C=10', 10), ('C=0.1', 0.1)):
clf = SVC(kernel='linear', C=penalty).fit(X, Y)

############# 이하 코드 생략 ###############

```
![](/assets/images/Supervised11_4.png)

- 'C=10'인 경우를 보면, 마진이 작아지는 한이 있더라도 슬랙변수를 만드려고 하지 않는다.
- 'C=0.1'인 경우를 보면, 마진이 커지게 되는 대신 슬랙변수가 만들어지게 된다. 이 경우 안으로 들어간 데이터 들은 **서포트 벡터**로 잡히게 된다.
- 판별함수를 파고 들어간 애들은 '서포트 벡터'가 된다. 

> 서포트 벡터가 '하나'이면 데이터가 변할 때마다 **성능분산**이 크게 차이가 난다. 서포트 벡터가 많아지면 이를 통한 **유사도 계산량**이 늘어나게 되는 단점이 있지만, 이에 따라 서포트 벡터가 하나일 때보다 '선'이 안정적이게 된다는 장점이 있다.

## 5. SVC를 활용한 이미지 인식
SVC는 이미지 recognition에도 사용된다. 아래에서는 mnist digits 이미지 데이터를 활용해 SVC모형으로 그려보도록 하겠다. (olivetti 이미지로도 활용 가능)

```py
### 이하 시각화 코드는 생략

## 0. 데이터 생성
from sklearn.datasets import load_digits
digits = load_digits()

## 1. train 데이터와 test데이터 split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    digits.data, digits.target, test_size=0.4, random_state=0)

## 2. SVC 모델 training    
from sklearn.svm import SVC
svc = SVC(kernel='linear').fit(X_train, y_train)

## 3. 모델 report 출력

from sklearn.metrics import classification_report, accuracy_score
y_pred_train = svc.predict(X_train)
y_pred_test = svc.predict(X_test)

print(classification_report(y_train, y_pred_train))
#                 f1-socore / support
# > accuracy: ... /   1.00  /   1078

print(classification_report(y_test, y_pred_test))
#                 f1-socore / support
# > accuracy: ... /   0.97  /   719

```
- 리포트를 보면 같은 'train'데이터에 대해서는 정확도가 100%가 나오고, 'test'데이터에 대해서도 정확도가 97%로 꽤나 잘 예측하는 모습을 보여준다.