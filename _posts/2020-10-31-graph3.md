---
title:  "[머신러닝] 그래프 모형 - 3. 네트워크 추론"
excerpt: "그래프 모형 중 네트워크 추론에서 변수제거 방법과 신뢰전파 방법"

categories:
  - GraphModel
tags:
  - UnSupervisedLearning
  - 10월
toc: true
toc_sticky: true
toc_label: 페이지 목차
use_math: true
---
> 아래 포스팅은 기존 수업의 복습 차원에서 올리는 포스팅입니다. 따라서 세부적인 수학적 정리가 생략된 부분이 있습니다. 따라서 좀 더 구체적인 정보나 원하시면 [데이터 사이언스 스쿨 사이트](https://datascienceschool.net/03%20machine%20learning/17.03%20%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC%20%EC%B6%94%EB%A1%A0.html#id4)를 참고 부탁드립니다. 특히 아래 코드를 이용한 시각화 그래프 코드와 모습을 보고 싶으시면 링크 확인부탁드립니다.  

## 0. 네트워크 추론
확률모형에서 일부 확률변수으 값이 주어졌을 때 다른 확률변수의 값이 얼마인지를 알아내는 것을 **추론(inference)**이라고 한다.

조건부 확률분포함수 $p(X_{\text{unknown}} \| X_\text{known})$를 알면 일부 확률변수의 값 $X_{\text{known}}$이 주어졌을 때 다른 확률변수 $X_{\text{unknown}}$의 확률 $p(X_{\text{unknown}})$을 알 수 있으므로 
**추론은 조건부 확률분포함수 $p(X_{\text{unknown}}\|\{X\}_{\text{known}})$를 알아내는 것**과 같다.

이전 포스팅에서 사용했던 건강상태의 따른 학생의 시험성적 예시를 다시 가져와 보도록 하겠다. 확률변수 A, B, C는 각각 학생의 건강상태, 공부시간, 시험성적이다. 

이 확률변수는 각각 {0, 1, 2}라는 값을 가질 수 있는데 이를 **하(0), 중(1), 상(2)**의 상태라고 하자.

```py
# 샘플 데이터
from IPython.core.display import Image
from networkx.drawing.nx_pydot import to_pydot

from pgmpy.factors.discrete import TabularCPD
from pgmpy.models import BayesianModel


P_A = TabularCPD('A', 3, [[0.1, 0.6, 0.3]])
P_B_I_A = TabularCPD('B', 3, 
    np.array([[0.6, 0.2, 0.2], [0.3, 0.5, 0.2], [0.1, 0.3, 0.6]]),
    evidence=['A'], evidence_card=[3])
P_C_I_B = TabularCPD('C', 3, 
    np.array([[0.8, 0.1, 0.1], [0.1, 0.8, 0.1], [0.1, 0.1, 0.8]]),
    evidence=['B'], evidence_card=[3])
                     
model = BayesianModel([('A', 'B'), ('B', 'C')])
model.add_cpds(P_A, P_B_I_A, P_C_I_B)

d = to_pydot(model)
d.set_dpi(300)
d.set_margin(0.2)
d.set_rankdir("LR")
Image(d.create_png(), width=600)

```

![](/assets/images/Graph2_1.png)

이 그래프 확률모형을 기반으로 다음과 같은 문제를 풀자.

1. 이 학생의 시험 성적이 어떤 확률분포를 가질 것인가? 어떤 성적을 맞을 확률이 가장 높은가?
2. 이 학생의 건강 상태가 좋았다. 어떤 성적을 맞을 확률이 가장 높은가?
3. 이 학생의 공부 시간이 적었지만 시험 성적은 좋았다. 건강 상태가 어땠을까?


베이지안 네트워크나 마코프 네트워크와 같은 그래프 확률모형에서 **추론**을 하려면 아래와 같은 방법을 사용한다.
- **변수제거** (variable elimination)
- **신뢰전파** (belief propagation) / 마코프 네트워크에서 사용

## 1. 변수제거
베이지안 네트워크에서 사용하는 방법으로, **값**을 알고 있는 확률변수 혹은 **무조건부 확률변수분포**를 알고있는 **확률변수부터 네트워크를 따라 차례대로 확률분포를 계산하는 방식**을 **변수제거(variable elimination)**방법이라고 한다.
위의 예시로 든 모형의 경우 **결합확률분포**는 아래와 같다.
$$P(A, B, C) = P(A)P(B|A)P(C|B)$$

우선 **특정한 확률변수의 무조건부 확률분포**를 구하는 방법을 보도록 하자. C분포함수를 알 때 B의 분포함수 중 'B=0'일 경우만 보자면 아래와 같이 구한다. 
$$P(B=0) = \sum_A P(B=0|A)P(A)$$

B=1일 때와 B=2일 때에도 같은 방법으로 구한다.  

다음으로 C의 분포함수를 계산해보면 다음과 같다. 여기서 $\sum_{A,B} = \sum_A \sum_B$이다.
$$P(C)= \sum_{A,B}P(C|B) P(B|A)P(A)$$


여기서 계산량을 줄여 인수분해로 묶은 식을 계산하면 P(C=0)의 경우 아래와 같이 구할 수 있다. 즉, 확률변수 B의 분포가 이미 계산된 상태라면 확률변수 **A의 영향이 없어진다.**
$$\begin{split}
P(C=0) 
&=& P(C=0|B=0) P(B=0) + \\
& & P(C=0|B=1) P(B=1) + \\
& & P(C=0|B=2) P(B=2) \\
\end{split})$$

### 코드 예시
pgmpy에서는 `VariableElimination` 클래스를 사용해 변수제거법을 적용할 수 있다. 생성자 인수로 네트워크 모형을 넣으며, `query`메서드를 지원한다.
- query(variable_list, evidence)
    - variable_list는 확률분포를 구하려는 확률변수의 리스트이고, evidence는 알고 있는 확률변수 값의 딕셔너리이다.

아무런 조건이 없을 경우 시험성적의 분포는 아래와 같다. 즉, '보통'의 성적을 받을 가능성이 가장 높다.

```py
from pgmpy.inference import VariableElimination

infer = VariableElimination(model)
print(infer.query(["C"])["C"])

'''
+-----+----------+
| C   |   phi(C) |
+=====+==========+
| C_0 |   0.2680 |
+-----+----------+
| C_1 |   0.3730 |
+-----+----------+
| C_2 |   0.3590 |
+-----+----------+
'''
```
위 값은 A, B순서로 `marginalize`한 것과 같다.
$$P(C)= \sum_{A} \sum_{B} P(A,B,C) =\sum_{A} \sum_{B}P(C|B)P(B|A)P(A)$$

```py
P_B = (P_B_I_A * P_A).marginalize(["A"], inplace=False)
P_C = (P_C_I_B * P_B).marginalize(["B"], inplace=False)
print(P_C)
'''
+-----+-------+
| C_0 | 0.268 |
+-----+-------+
| C_1 | 0.373 |
+-----+-------+
| C_2 | 0.359 |
+-----+-------+
'''
```

만약 건강상태가 괜찮다면 `evidence={"A": 2} `인수를 적용한다. 이럴 경우 좋은 성적을 받을 **가능성**이 가장 높다. 이 결과는 A=2라는 확률분포에서 `marginalize`한 것과 같다.

```py
print(infer.query(["C"], evidence={"A": 2})["C"])
'''
+-----+----------+
| C   |   phi(C) |
+=====+==========+
| C_0 |   0.2400 |
+-----+----------+
| C_1 |   0.2400 |
+-----+----------+
| C_2 |   0.5200 |
+-----+----------+
'''
######################
P_A2 = TabularCPD('A', 3, [[0, 0, 1]])
P_B = (P_B_I_A * P_A2).marginalize(["A"], inplace=False)
P_C = (P_C_I_B * P_B).marginalize(["B"], inplace=False)
print(P_C)
'''
+-----+------+
| C_0 | 0.24 |
+-----+------+
| C_1 | 0.24 |
+-----+------+
| C_2 | 0.52 |
+-----+------+
'''
```

만약 **시험성적(C)**과 **공부 시간(B)**을 알고 있다면 건강상태(A)를 **유추**할 수도 있다. 하지만 B를 알고 있는 경우 어차피 A와 C는 **서로 독립**이다. 따라서 공부시간을 알고 있다면 **시험성적과 관계없이 건강상태를 유추**할 수 있다.
```py
print(infer.query(["A"], evidence={"B": 0, "C": 2})["A"])
'''
+-----+----------+
| A   |   phi(A) |
+=====+==========+
| A_0 |   0.2500 |
+-----+----------+
| A_1 |   0.5000 |
+-----+----------+
| A_2 |   0.2500 |
+-----+----------+
'''

print(infer.query(["A"], evidence={"B": 0})["A"])
'''
B만으로도 위와 같이 A를 유추할 수 있다.
+-----+----------+
| A   |   phi(A) |
+=====+==========+
| A_0 |   0.2500 |
+-----+----------+
| A_1 |   0.5000 |
+-----+----------+
| A_2 |   0.2500 |
+-----+----------+
'''
```

위 추론은 조건부확률 $P(A|B)$를 구하는 것과 같다. 
$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$
여기서 $P(B)$는 동일하므로 무시할 수 있다.
$$P(A|B) \propto P(B|A)P(A)$$

```py
print(P_B_I_A * P_A)
'''
+-----+----------------------+------+------+
| A   | A_0                  | A_1  | A_2  |
+-----+----------------------+------+------+
| B_0 | 0.06                 | 0.12 | 0.06 |
+-----+----------------------+------+------+
| B_1 | 0.03                 | 0.3  | 0.06 |
+-----+----------------------+------+------+
| B_2 | 0.010000000000000002 | 0.18 | 0.18 |
+-----+----------------------+------+------+
'''

# 특정한 값에 대한 조건 확률을 구하기 위해 factor로 바꾼 뒤 reduce메소드를 수행한다.
print((P_B_I_A * P_A).to_factor().reduce([("B", 0)], 
            inplace=False).normalize(inplace=False))
'''
+-----+----------+
| A   |   phi(A) |
+=====+==========+
| A_0 |   0.2500 |
+-----+----------+
| A_1 |   0.5000 |
+-----+----------+
| A_2 |   0.2500 |
+-----+----------+
'''
```
> 변수제거 문제는 '몬티홀 문제'에 이용할 수 있다.

## 2. 신뢰전파
예측문제(predicion)의 경우은 내가 입력하는 것과 나오는 것이 정해져있는 경우가 많다. 하지만 그래프 형식은 뭐를 집어넣고, 뭐를 출력할 지에 대해서 정해져 있지 않다. 따라서 몇 가지 변수를 통해 특정 변수를 알아낼 수 있다. 따라서 그래프의 경우 **일반적인 지도학습과는 다르다고 할 수 있다**.  

**신뢰전파**(belif propagation)방법은 **메시지 전달(message passin)방법**이라고도 한다. 여기에서는 선형 체인 형태의 마코프 네트워크를 예로 들어 설명하지만 일반적인 형태의 네트워크에서도 성립한다.  

$X_1, \dots, X_N$의 $N$개 확률변수가 선형 체인으로 연결된 마코프 네트워크에서 결합확률분포는 다음과 같다.
$$
p(X_1, \ldots, X_N) = \dfrac{1}{Z}\psi(X_1, X_2) \psi(X_2, X_3) \cdots \psi(X_{N-1}, X_{N}) 
$$

위 체인의 중간에 있는 $X_N$의 확률분포를 알아내려면 **전체확률의 법칙을 사용**해 $X_n$을 제외한 나머지 확률변수들이 가질 수 있는 모든 경우의 확률을 더한다. 이 수식을 정리하면 아래와 같이 쓸 수 있다.
$$p(X_n) = \frac{1}{Z}\mu_{\alpha}(X_n)\mu_{\beta}(X_n)$$
이 식에서 $\mu_{\alpha}, \mu_{\beta}$는 $X_n$에 도달하는 좌측, 우측 메시지 함수라고 한다.
-$X_n$에 대한 함수이기 때문에 **1차원 함수**이다.  

**메시지함수는 재귀적으로 계산**할 수 있다.
$$
\begin{eqnarray}
\mu_{\alpha}(X_n)
&=& \sum_{X_{n-1}} \psi(X_{n-1}, X_n) \mu_{\alpha}(X_{n-1}) \\
\mu_{\alpha}(X_2) &=& \sum_{X_1} \psi(X_1, X_2)
\end{eqnarray}
$$

### 코드예시

pgmpy에서는 `BeliefPropagation` 클래스를 사용하여 신뢰전파법을 적용할 수 있다. 사용법은 위 `VariableElimination`과 같다. 앞서 사용했던 건강 상태 및 시험 성적과 관련된 예제를 **마코프 네트워크로 변형**해서 신뢰전파를 적용해보고자 한다.

![](/assets/images/Graph3_1.jpg){: .align-center}

```py
# 0. 마코프 네트워크로 변형한 모델
# 앞선 예제 데이터 그대로 사용
model2 = model.to_markov_model()

from pgmpy.inference import BeliefPropagation
infer = BeliefPropagation(model)
print(infer.query(["C"])["C"])
'''
+-----+----------+
| C   |   phi(C) |
+=====+==========+
| C_0 |   0.2680 |
+-----+----------+
| C_1 |   0.3730 |
+-----+----------+
| C_2 |   0.3590 |
+-----+----------+
'''

# 1. 건강상태가 좋은 (A=2) 경우
print(infer.query(["C"], evidence={"A": 2})["C"])
'''
+-----+----------+
| C   |   phi(C) |
+=====+==========+
| C_0 |   0.2400 |
+-----+----------+
| C_1 |   0.2400 |
+-----+----------+
| C_2 |   0.5200 |
+-----+----------+
'''

# 2. 공부를 많이 못했음에도 성적이 좋았을 경우, A의 값
print(infer.query(["A"], evidence={"B": 0, "C": 2})["A"])
'''
+-----+----------+
| A   |   phi(A) |
+=====+==========+
| A_0 |   0.2500 |
+-----+----------+
| A_1 |   0.5000 |
+-----+----------+
| A_2 |   0.2500 |
+-----+----------+
'''
```

