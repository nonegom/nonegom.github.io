---
title:  "[머신러닝] 그래프 모형 - 4. 가우시안 혼합모형과 EM방법"
excerpt: "그래프 모형 중 네트워크 추론에서 변수제거 방법과 신뢰전파 방법"

categories:
  - GraphModel
tags:
  - UnSupervisedLearning
  - 11월
toc: true
toc_sticky: true
toc_label: 페이지 목차
use_math: true
---

> 아래 포스팅은 기존 수업의 복습 차원에서 올리는 포스팅입니다. 따라서 세부적인 수학적 정리가 생략된 부분이 있습니다. 따라서 좀 더 구체적인 정보나 원하시면 [데이터 사이언스 스쿨 사이트](https://datascienceschool.net/03%20machine%20learning/18.01%20%EA%B0%80%EC%9A%B0%EC%8B%9C%EC%95%88%20%ED%98%BC%ED%95%A9%EB%AA%A8%ED%98%95%EA%B3%BC%20EM%20%EB%B0%A9%EB%B2%95.html)를 참고 부탁드립니다. 특히 아래 코드를 이용한 시각화 그래프 코드와 모습을 보고 싶으시면 링크 확인부탁드립니다.  

## 0. 가우시안 혼합모형
$K$-클래스 카테고리 확률변수 $Z$가 있다고 할 때, 실수값을 출력하는 확률변수 $X$는 확률변수 $Z$의 표본값 $k$에 따라 기댓값 $\mu_k$, 분산 $\Sigma_k$가 달라진다.  

이 때 $p(x)$의 값은 다음과 같다.
$$ p(x) = \sum_Z p(z)p(x\mid z) = \sum_{k=1}^{K} \pi_k \mathcal{N}(x \mid \mu_k, \Sigma_k) $$

실수값을 출력하는 확률변수 $X$가 $K$-클래스 카테고리 확률변수 $Z$의 값에 따라 **다른 기댓값과 분산을 가지는 복수의 가우시안 정규분포들**로 이루어진 모형을 **가우시안 혼합 모형**(Gaussian Mixture)이라고 한다.


### 베르누이-가우시안 혼합모형
카테고리가 두 개인 가우시안 혼합모형은 베르누이-가우시안 혼합모형(Bernouilli Gaussian-Mixuture Model)이라고 한다.

![](/assets/images/Graph4-1.JPG)

- 데이터만 보고 **히든 스테이트의 변수**를 찾아내고, **봉우리 각각의 시그마값과 뮤값**을 찾아내야 한다.

아래는 2개의 카테고리와 2차원 가우시안 정규분포를 가지는 가우시안 혼함모형 데이터의 예이다.

```py
# 가우시안 혼합모형 데이터 생성

from numpy.random import randn

n_samples = 500

mu1 = np.array([0, 0])
mu2 = np.array([-6, 3])
sigma1 = np.array([[0., -0.1], [1.7, .4]])
sigma2 = np.eye(2)

np.random.seed(0)
X = np.r_[1.0 * np.dot(randn(n_samples, 2), sigma1) + mu1,
          0.7 * np.dot(randn(n_samples, 2), sigma2) + mu2]
plt.scatter(X[:, 0], X[:, 1], s=5)
plt.xlabel("x1")
plt.ylabel("x2")
plt.title("베르누이 가우시안 혼합 모형의 표본")
plt.show()
```

![](/assets/images/Graph4_2.png)

- 여기서 우리는 첫 번째 파라미터 $\theta$를 알아내야 한다. 즉, 중심위치와 공분산 행렬을 찾아낸다.


### 가우시안 혼합모형의 모수 추정

데이터로부터 **가우시안 혼합모형의 모수를 추정**한다는 것은 관측되지 않는 카테고리 분포의 확률분포와 **각각의 카테고리**에서의 가우시안 정규분포 모수를 모두 추정하는 것을 말한다.  
이 때 어려운 점은 확률분포함수가 선형대수방법으로 쉽게 구할 수 없는 복잡한 형태가 된다.  

N개의 데이터에 대한 X의 확률분포에 로그를 취하면
$$  \log p(x) = \sum_{i=1}^N \log \left( \sum_{k=1}^{K} \pi_k \mathcal{N}(x_i\mid \mu_k, \Sigma_k) \right) $$
- 여기서 두 식 모두 미분값이 0이 되는 모수를 쉽게 구하기 힘들다. 여기서 파이, 뮤, 시그마가 'theta($\theta$)'이다.   


데이터 $x_i$가 어떤 카테고리 $z_i$에 속하는지를 알 경우 같은 카테고리에 속하는 데이터만 모아서 **카테고리 확률분포 $\pi_k$**도 알 수 있고, **가우시안 정규분포의 모수 $\mu_k, \Sigma_k$**도 쉽게 구할 수 있다. 하지만 실제로는 데이터 $x_i$가 가지고 있는 **카테고리 값 $z_i$를 알 수가 없기 때문**에 위 확률분포함수를 최대화하는 $\pi_k$와 $\mu_k, \Sigma_k$를 **비선형 최적화**를 통해 구해야 한다.

네트워크 확률모형 관점에서는 확률변수 $Z_i$가 확률변수 $X_i$에 영향을 미치는 단순한 모형이다. 하지만 $i=1,\dots, N$ 인 모든 경우에 대해 반복적으로 영향을 미치므로 아래와 같이 **판넬모형**으로 표현한다.

![](/assets/images/Graph4_3.JPG)