---
title:  "[머신러닝] 그래프 모형 - 2. 그래프 확률모형"
excerpt: "확률적 그래프 모형 중 베이지안 네트워크와 마코프 네트워크"

categories:
  - GraphModel
tags:
  - UnSupervisedLearning
  - 10월
toc: true
toc_sticky: true
toc_label: 페이지 목차
use_math: true
---
> 아래 포스팅은 기존 수업의 복습 차원에서 올리는 포스팅입니다. 따라서 세부적인 수학적 정리가 생략된 부분이 있습니다. 따라서 좀 더 구체적인 정보나 원하시면 [데이터 사이언스 스쿨 사이트](https://datascienceschool.net/03%20machine%20learning/17.02%20%EA%B7%B8%EB%9E%98%ED%94%84%20%ED%99%95%EB%A5%A0%EB%AA%A8%ED%98%95.html)를 참고 부탁드립니다. 특히 아래 코드를 이용한 시각화 그래프 코드와 모습을 보고 싶으시면 링크 확인부탁드립니다.  

## 0. 그래프 확률모형
여러 확률변수의 결합분포를 구해야 하는 경우를 가정하자. A, B, C 3개의 확률변수가 있고 각 확률변수 0, 1, 2 세가지의 값만 가질 수 있는 카테고리 확률변수인 경우라고 가정한다. 그렇다면 총 ($3^3$에서 -1한) 26개의 모수가 필요하다. (총 합이 1이 되어야 하므로 나머지 26개를 구하면, 1개는 **1 - (26개의 확률 값)**으로 구할 수 있다.)  

이 정보들은 우리가 알 수 있는 최대한의 정보들이다. 하지만 독립이거나 조건부 독립이 되는 경우가 있는 **결합 확률 분포**에서는 해당 정보들이 다 필요가 없을 수도 있다. 


- 1차원에서는 2개 (3-1)
- 2차원에서는 8개 (9-1)
- 3차원에서는 26개 (27-1)
의 모수가 필요하다. 


## 1. 베이지안 네트워크 모형
현실에서는 모든 확률변수가 서로 영향을 미치는 복잡한 경우보다 **특정한 몇 개의 확률분포들**이 서로 영향을 미치는 경우가 더 자주 나타난다. 

### 예시
예를 들어 A, B, C가 각각 어떤 학생의 'A: 건강 상태', 'B: 공부 시간', 'C: 시험 성적'을 나타낸다고 하자.

- '공부시간 B'는 '시험성적 C'에 영향을 미친다.
- '건강상태 A'는 '공부시간 B'와 인과관계가 있지만, C와는 직접적인 인과관계가 없다.  

이렇게 다수의 확률변수 중 특정한 소수의 확률변수들이 가지는 관계를 그래프로 표현한 것을 **그래프 확률모형(graphical probability model)**이라고 한다. 그리고 그 중에서도 인과관계가 확실하여 **방향성 그래프**로 표시할 수 있는 것을 **베이지안 네트워크 모형**이라고 한다.

```py
import networkx as nx
from IPython.core.display import Image
from networkx.drawing.nx_pydot import to_pydot

g1 = nx.DiGraph()
g1.add_path(["A", "B", "C"])
d1 = to_pydot(g1)
d1.set_dpi(300)
d1.set_rankdir("LR")
d1.set_margin(0.2)
Image(d1.create_png(), width=600)
```

![](/assets/images/Graph2_1.png){: width="500"}


여기서 A와 C도 상관관계와 인과관계가 있긴 하지만, **직접적인 관계**는 없다. 이러한 관계를 **조건부 독립**이라고 한다. 이러한 조건부 독립의 모양을 그래프로 그릴 수 있다. 


### 방향성 그래프
---
방향성 그래프에서 **확률변수는 하나의 노드(node)** 또는 정점(vertex)로 나타내고 **인과관계는 화살표 간선(edge, link)**으로 나타낸다.  
방향성 그래프 모델에서는 원인과 결과가 되는 두 확률변수의 관계를 조건부 확률분포로 표현한다. 위의 모형에서처럼 A가 B의 원인이 된다면 이 두 확률변수의 관계를 $P(B|A)$로 나타내고 B가 C의 원인이 되므로 두 확률변수의 관계는 P(C|B)로 나타낸다. (원인에 해당하는 독립변수가 뒤로 간다.)  

그리고 **전체 확률변수들 간의관계**는 이러한 조건부 확률분포를 결합하여 나타낼 수 있다. 위의 그래프에서 전체결합분포는 다음과 같다.
$$P(A,B,C) = P(A)P(B|A)P(C|B)$$

**전체 결합분포**의 경우 총 26개의 변수를 알아야 하기에, 저장공간 또한 26개가 필요하다. 하지만 **조건부 확률 분포**의 경우 우리가 알아야 하는 모수의 수는 **14**개밖에 되지 않는다. 

![](/assets/images/Graph2_2.JPG){: width="500", height="500"}{: .align-center}

- $P(A): 3 - 1 =2$개
- $P(B\|A): 3 \times 2= 6$개
- $P(C\|B): 3 \times 2= 6$개


따라서 **변수간의 인과 관계**라는 추가 정보로 인해 모수의 숫자가 26개에서 14개로 크게 감소한다. 이렇게 우리가 알고 있는 확률변수 간의 정보를 그래프에 추가하면 문제를 더 간단하게 만들 수 있다.  

> 이런 식으로 모수의 개수를 줄이면서 사용하기 위해서는 그래프를 통해 각 변수의 인과관계를 알아내야 한다. 따라서 **그래프를 이용**한다. 

## 코드 예시
pgmpy 패키지에서는 `TabularCPD` 클래스로 조건부확률분포를 구현할 수 있다.


위 예제의 **조건부 확률 분포**를** 파이썬으로 구현해 보자. 우선 **건강 상태(A)**는 나쁠(A=0) 확률이 10%, 보통(A=1)일 확률이 60%, 좋을 확률이 30%라고 가정한다.

1. 건강 상태 조건 A에 따른 **공부시간 B**의 확률분포는 다음과 같다.
    - 건강 상태가 나쁘면(A=0), 공부시간이 적거나(B=0), 보통이거나(B=1), 많을(B=2) 확률은 각각 60%, 30%, 10%다.
    - 건강 상태가 보통이면(A=1), 공부시간이 적거나(B=0), 보통이거나(B=1), 많을(B=2) 확률은 각각 20%, 60%, 20%다.
    - 건강 상태가 좋으면(A=2), 공부시간이 적거나(B=0), 보통이거나(B=1), 많을(B=2) 확률은 각각 20%, 30%, 50%다.


2. 공부시간 B에 따른 **성적 C**의 확률분포는 다음과 같다.
    - 공부시간이 적으면(B=0), 성적이 나쁘거나(C=0), 보통이거나(C=1), 좋을(C=2) 확률은 각각 80%, 10%, 10%이다.
    - 공부시간이 보통이면(B=1), 성적이 나쁘거나(C=0), 보통이거나(C=1), 좋을(C=2) 확률은 각각 10%, 80%,10%이다.
    - 공부시간이 많으면(B=2), 성적이 나쁘거나(C=0), 보통이거나(C=1), 좋을(C=2) 확률은 각각 10%, 10%, 80%이다.


```py
## A의 확률 
from pgmpy.factors.discrete import TabularCPD
P_A = TabularCPD('A', 3, [[0.1], [0.6], [0.3]])
print(P_A)
'''
+-----+-----+
| A_0 | 0.1 |
+-----+-----+
| A_1 | 0.6 |
+-----+-----+
| A_2 | 0.3 |
+-----+-----+
'''

## B|A의 확률
P_B_I_A = TabularCPD('B', 3, np.array([
        [0.6, 0.2, 0.2], [0.3, 0.5, 0.2], [0.1, 0.3, 0.6]]),
        evidence=['A'], evidence_card=[3])
print(P_B_I_A)

# 조건이 위쪽(열)으로 오고, 확률분포가 왼쪽(행)으로 온다.
# 주의해야할 점은 확률을 타이핑할 때에는 행순서로 한다.
'''
+------+------+------+------+
| A    | A(0) | A(1) | A(2) |
+------+------+------+------+
| B(0) | 0.6  | 0.2  | 0.2  |
+------+------+------+------+
| B(1) | 0.3  | 0.5  | 0.2  |
+------+------+------+------+
| B(2) | 0.1  | 0.3  | 0.6  |
+------+------+------+------+
  합  :    1      1      1
'''

# A와 B의 결합확률(joint probability)을 구할 수 있다.
## B(0), A(0)일 때의 확률은 A=0일때와 B|A=0일때의 확률의 곱 

print(P_B_I_A * P_A)
'''
+------+----------------------+------+------+
| A    | A(0)                 | A(1) | A(2) |
+------+----------------------+------+------+
| B(0) | 0.06                 | 0.12 | 0.06 | - B가 0일 때의 확률
+------+----------------------+------+------+
| B(1) | 0.03                 | 0.3  | 0.06 |
+------+----------------------+------+------+
| B(2) | 0.010000000000000002 | 0.18 | 0.18 |
+------+----------------------+------+------+

계산법) A=0일 때 '0.1' X A=0일 때 B=0일 확률 '0.6'을 곱해서 0.06이 나온다. 
'''
```

### `marginize`
CPD 객체는 `marginalize` 메서드로 특정 변수의 모든 경우의 확률을 더하는 sum-out을 할 수 있다. (전체 확률의 법칙)
$$P(B) = \sum_A P(A,B) = \sum_A P(B|A)P(A)$$

- 위에서 구한 **결합확률 값**을 더한다.

```py
# B의 확률값 구하기 
P_B = (P_B_I_A * P_A).marginalize(["A"], inplace=False)
print(P_B)

'''
+-----+------+
| B_0 | 0.24 |
+-----+------+
| B_1 | 0.39 |
+-----+------+
| B_2 | 0.37 |
+-----+------+
'''
```

이렇게 B의 확률값을 구하는 것을 **'추론'**이라고 한다. 위 방법을 이용하면 'C|B'를 이용해서 'C'의 확률을 추론할 수 있다. 
```py
P_C_I_B = TabularCPD('C', 3, np.array(
          [[0.8, 0.1, 0.1], [0.1, 0.8, 0.1], [0.1, 0.1, 0.8]]),
          evidence=['B'], evidence_card=[3])
print(P_C_I_B)
```


### add_cpds
이 조건부 확률들을 결합해 하나의 베이지안 네트워크로 마들려면 `BayesianModel`클래스를 사용한다. 생성자에는 노드를 연결한 그래프를 넣고 `add_cpds`메서드로 **조건부 확률**을 추가할 수 있다.

```py
from pgmpy.models import BayesianModel
model = BayesianModel([('A', 'B'), ('B', 'C')])
model.add_cpds(P_A, P_B_I_A, P_C_I_B)
model.check_model()
# > True

from IPython.core.display import Image
from networkx.drawing.nx_pydot import to_pydot
d = to_pydot(model)
d.set_dpi(300)
d.set_margin(0.2)
d.set_rankdir("LR")
Image(d.create_png(), width=600)
```

![](/assets/images/Graph2_3.png)


이렇게 만들어진 모형으로부터 **여러가지 추론(inference)**을 할 수 있다.  
결합확률분포 함수를 찾고 그 함수로부터 A, B, C의 `marginal` 확률분포를 계산하면 A, B, C의 값으로 어떤 값이 가장 확률이 높은지 알 수 있다. (추론에 사용된 `VariableElimination` 클래스의 사용법에 대해서는 추후에 학습). 분석 결과를 보면 시험 성적이 좋을 확률은 35.9%이다.

```py
from pgmpy.inference import VariableElimination
inference = VariableElimination(model)
result = inference.query(variables=["C"])
print(result["C"])
'''
+-----+----------+
| C   |   phi(C) |
+=====+==========+
| C_0 |   0.2680 |
+-----+----------+
| C_1 |   0.3730 |
+-----+----------+
| C_2 |   0.3590 |
+-----+----------+
'''
```

## 2. 베이지안 네트워크의 결합확률분포
베이지안 네트워크를 만들려면 조사 대상이 되는 확률변수를 노드(node)로 생성하고 인과관계가 있는 노드를 방향성 간선(directed edge)로 연결한다. 베이지안 네트워크를 구성하는 확률변수의 결합확률분포는 다음처럼 주어진다.
$$P(X_1, \cdots, X_N) = \prod_{i=1}^N P(X_i|Pa(X_i))$$

이 식에서 $Pa(X_i)$는 $X_i$의 부모 노드이다. 
- 즉, 부모노드를 원인으로 하고, 자식노드를 결과로 하는 확률들의 곱이라고 할 수 있다. 위는 **'정리'**이므로 그냥 **외우면 된다**. 

- 조건) 만약 넘버링에서 **넘버가 크면 선조**, **넘버가 작으면 후손**이 되어야 한다. 즉 i < j이면, i는 j의 후손격이 된다.

```py
g1 = nx.DiGraph()
g1.add_edge("X1", "X3")
g1.add_edge("X1", "X4")
g1.add_edge("X3", "X4")
g1.add_edge("X2", "X4")
g1.add_edge("X2", "X7")
g1.add_edge("X4", "X5")
g1.add_edge("X4", "X6")

d1 = to_pydot(g1)
d1.set_dpi(300)
d1.set_margin(0.2)
d1.set_rankdir("LR")
Image(d1.create_png(), width=800)
```

![](/assets/images/Graph2_4.png)

예를 들어 $X_1, \dots, X_6$의 관계가 위 그래프와 같다면 결합확률분포는 다음과 같다.
$$P(X_1, \dots ,X_7) = P(X_1)P(X_2)P(X_3|X_1)P(X_4|X_1,X_2,X_3)P(X_5|X_4)P(X_6|_X4)P(X_7|X_2)$$


- 곱할 때에는 넘버링된 숫자대로 곱해야 한다. **P(자식ㅣ부모(복수 가능))**

### 조건부 독립
베이지안 네트워크를 만들 때 중요한 것은 확률번수간의 조건부 독립 관계가 그래프에 나타나고 있어야 한다.

조건부 독립은 **조건이 되는 확률변수가 존재**해야 한다. 일반적으로 확률변수 A, B가 독립인 정의는 $P(A, B) = P(A)P(B)$이다. 조건부 독립은 조건이 되는 $C$라는 확률변수에 대한 **조건부 결합확률분포**에 대해 다음이 만족 되어야 한다. 
$$P(A,B|C) = P(A|C)P(B|C)$$

즉, $C$에 대한 조건부 결합확률분포가 **조건부 확률분포의 곱**으로 나타난다.  

A, B가 C에 대해 조건부 독립이면 다음도 만족한다. (★반드시 외워야 하는 조건부 독립의 성질)
$$P(A|B,C) = P(A|C)$$
$$P(B|A,C) = P(B|C)$$

- C가 정해졌다면, A나 B가 필요 없게된다.
- 주의할 점은 조건부 독립과 (무조건부)독립은 관계가 없다.

### 방향성 분리
---
**방향성 분리 정리** (directed separation)정리는 방향성 그래프 모형에서 어떤 **두 노드(확률변수)가 조건부 독립인지 아닌지** 알아보는 방법이다. 이를 위해서는 세 가지 간선 결합을 알아야 한다.

- 꼬리 - 꼬리 결합
- 머리 - 꼬리 결합
- 머리 - 머리 결합

#### 1) 꼬리 - 꼬리 결합
확률변수 A, B가 **공통의 부모 C**를 가지는 경우, C에서는 간선(화설표)의 꼬리가 2개 붙어 있기 때문에 C는 **꼬리-꼬리(tail-to-tail)결합**이라고 한다. 

![](/assets/images/Graph2_5.JPG){: .align-center}

이때 A와 B는 독립은 아니지만 **조건부 독립**이 성립한다
$$P(A,B|C) = \frac{P(A,B,C)}{P(C)} = \frac{P(A|C)P(B|C)P(C)}{P(C)} = P(A|C) P(B|C)$$

즉, C가 어떤 값인지 알고 있을 때 A의 확률은 B의 값과 아무런 관계가 없다. (B의 경우도 마찬가지이다.) 

#### 2) 머리 - 꼬리 결합

인과관계인 확률변수 A, B사이에 **C가 끼어 있는 경우**, 노드 C에서 간선의 머리와 꼬리가 만나기 때문에 **머리-꼬리(head-to-tail) 결합**이라고 한다.

![](/assets/images/Graph2_6.JPG)

이 경우에도 A와 B는 독립이 아니지만 **조건부 독립**이 성립한다. (베이즈 정리)
$$P(A,B|C) = \frac{P(A,B,C)}{P(C)} = \frac{P(A)P(C|A)P(B|C)}{P(C)} = \frac{P(C)P(A|C)P(B|C)}{P(C)} = P(A|C) P(B|C)$$

즉, C가 어떤 값인지 알고 있을 때 A의 확률은 B의 값과 아무런 관계가 없다. (B의 경우도 마찬가지이다.) 

#### 3) 머리 - 머리 결합

A, B 두 **부모를 가지는 C**가 있는 경우, 이러한 구조는 **V-구조**(V-structure) 또는 **머리-머리(head-to-head)결합**이라고 한다.

![](/assets/images/Graph2_7.JPG){: .align-center}

이 경우에서는 앞의 두 경우와 달리 **A와 B가 독립**이 된다. 그렇지만 조건부 독립은 성립하지 않고, 대신 C값을 알고 있다면 **A와 B는 종속관계**가 된다.  

예를 들어 A가 늦잠을 잘 확률, B가 길이 막힐 확률, C가 지각하는 확률을 나타냈다고 가정할 경우, 늦잠을 자는 것(A)과 길이 막히는 것(B)은 **서로 독립**이다. 하지만 만약 지각(C)이 발생한 상황에서는 A와 B는 서로 독립이 아니며 이 경우 **반-상관관계**를 가진다. 즉 이미 지각을 한 상황에서 늦잠을 자지 않았다면 길이 막혔을 가능성이 높아지게 되고, 길이 막히지 않았다면 늦잠을 잤을 가능성이 높아진다. 이러한 것을 explaining-out이라고 한다.  

이러한 상황은 **C**가 A, B의 직집적인 자식이 아니라 **후손인 경우에도 성립**한다.

![](/assets/images/Graph2_8.png){: width="400", height="500"}{: .align-center}


이상의 상화을 정리한 것이 바로 **'방향성 분리 정리'**이다. 방향성 분리 정리에 따르면 **A와 B가 C에 대해서 조건부 독립인 경우**는 다음 조건이 만족되어야 한다.

1. C가 A, B 사이의 경로에 있는 **꼬리-꼬리 결합**이거나 **머리-꼬리 결합**이다.
2. C가 A, B 사이의 경로상에 있는 **머리-머리 결합**이 아니거나 혹은 이러한 노드의 자손이 아니어야 한다.

- '경로'에서는 화살표의 의미가 없다. 여결만 되어있으면 '경로'가 된다. 단 경로가 여러 개일 경우 모든 경우에 대해 알아야 한다.

## 3. 마코프 네트워크

변수 간의 인과관계가 순환(cycle)관계를 이루기 때문에 방향성이 있는 베이지안 네트워크로 구현할 수 없는 경우도 있다. 이때는 무방향성 그래프인 **마코프 네트워크**(Markov Network)를 사용한다. **마코프 무작위장**(Markov Random Field)라고도 한다.  

3X3이미지의 픽셀 9개의 값이 확률변수라고 할 경우, 이 9개의 확률변수 중 어떤 2개라도 서로 관련성을 갖게 되는데, 이 때 아래와 같은 마코프 네트워크를 사용할 수 있다.

![](/assets/images/Graph2_9.png)

### 클리크와 팩터 
마코프 네트워크는 **클리크(clique)**로 구성되는데 클리크를 구성하는 확률변수의 분포는 **포텐셜 함수** 또는 **팩터(factor)**로 표현할 수 있다. **팩터(factor)**는 가능한 모든 결과의 조합에 대한 **실수함수**다. 따라서** 확률분포함수는 팩터의 일종**이다. 하지만 팩터는 확률분포함수와 달리 모든 값을 더해서 1이 되어야 한다는 조건이 없다.
$$ p(X,Y) = \frac{1}{Z}\psi_1(X,Y)$$
$$ p(X|Y) = \frac{1}{Z}\psi_2(X,Y)$$


pgmpy는 팩터를 정의하는 `DiscreteFactor`클래스를 제공한다. **CPD 결합분포 객체**(조건부 확률 분포)는 **팩터로 변환**하는 `to_factor`메서드를 제공한다. 

```py

# 그림과 같은 모형을 그래프화
from pgmpy.factors.discrete import DiscreteFactor
phi = DiscreteFactor(['x1', 'x2', 'x3'], [2, 2, 2], np.arange(8))
print(phi)

'''
+------+------+------+-----------------+
| x1   | x2   | x3   |   phi(x1,x2,x3) |
+======+======+======+=================+
| x1_0 | x2_0 | x3_0 |          0.0000 |
+------+------+------+-----------------+
| x1_0 | x2_0 | x3_1 |          1.0000 |
+------+------+------+-----------------+
| x1_0 | x2_1 | x3_0 |          2.0000 |
+------+------+------+-----------------+
| x1_0 | x2_1 | x3_1 |          3.0000 |
+------+------+------+-----------------+
| x1_1 | x2_0 | x3_0 |          4.0000 |
+------+------+------+-----------------+
| x1_1 | x2_0 | x3_1 |          5.0000 |
+------+------+------+-----------------+
| x1_1 | x2_1 | x3_0 |          6.0000 |
+------+------+------+-----------------+
| x1_1 | x2_1 | x3_1 |          7.0000 |
+------+------+------+-----------------+
'''

# 결과의 조합에 대한 실수함수
print(P_B_I_A.to_factor())
'''
  결과  조건
+-----+-----+------------+
| B   | A   |   phi(B,A) |
+=====+=====+============+
| B_0 | A_0 |     0.6000 |
+-----+-----+------------+
| B_0 | A_1 |     0.2000 |
+-----+-----+------------+
| B_0 | A_2 |     0.2000 |
+-----+-----+------------+
| B_1 | A_0 |     0.3000 |
+-----+-----+------------+
| B_1 | A_1 |     0.5000 |
+-----+-----+------------+
| B_1 | A_2 |     0.2000 |
+-----+-----+------------+
| B_2 | A_0 |     0.1000 |
+-----+-----+------------+
| B_2 | A_1 |     0.3000 |
+-----+-----+------------+
| B_2 | A_2 |     0.6000 |
+-----+-----+------------+
'''
```

factor 객체는 어떤 확률변수가 특정한 값을 가지는 경우만 추출하는 `reduce` 메서드를 제공한다.

```py
# B = 0일 경우만 잘라냄
print(P_B_I_A.to_factor().reduce([("B", 0)], inplace=False))
'''
+-----+----------+
| A   |   phi(A) |
+=====+==========+
| A_0 |   0.6000 |
+-----+----------+
| A_1 |   0.2000 |
+-----+----------+
| A_2 |   0.2000 |
+-----+----------+
'''

# A = 1일 경우만 잘라냄
print(P_B_I_A.to_factor().reduce([("A", 1)], inplace=False))
'''
+-----+----------+
| B   |   phi(B) |
+=====+==========+
| B_0 |   0.2000 |
+-----+----------+
| B_1 |   0.5000 |
+-----+----------+
| B_2 |   0.3000 |
+-----+----------+
'''
```

## 4. 마코프 네트워크의 확률분포
마코프 네트워크의 결합확률분포는 **마코프 네트워크를 구성하는 모든 클리크의 팩터의 곱**으로 나타난다.
$$P(X) = \frac{1}{Z(X)} \prod_{\{C\}} \psi_C(X_C)$$

이 식에서 $C$는 **클리크**, $X_C$는 그 **클리크 안의 확률변수**, $\psi_C$는 그 **클리크의 팩터**, {$C$}는 **모든 클리크의 집합**, $Z$는 **파티션 함수**(partition)를 나타낸다.  

예를 들어 3x3이미지의 경우 9개의 확률변수의 결합확률분포는 다음처럼 표시할 수 있다.
$$P(X_{11}, \dots, X_{33}) = \frac{1}{Z} \prod \psi(X_{11}, X_{12}) \psi(X_{11}, X_{21}) \psi(X_{12}, X_{13})\ \cdots\ \psi(X_{23}, X_{33})\psi(X_{32}, X_{33})$$


### 에너지 함수
팩터 함수는 아래와 같이 표현할 수 있다.
$$\psi(X) = exp(-E(X))$$

이 식에서 $E(X)$를 에너지 함수라고 한다. 확률이 높을수록 에너지 함수의 값은 작아지게 된다.  pgmpy에서는 `MarkovModel`클래스로 마코프 네트워크 모형을 구현한다. 

![](/assets/images/Graph2_9.png)


팩터는 `DiscreteFactor` 클래스로 구현할 수 있다. 만약 $X_{11}, X_{12}$의 팩터함수가 아래의 표와 같다면 코드는 다음과 같이 구현한다.

  | |$X_{12} = 0$| $X_{12} = 1$ |
  |---|:---|:----|
  |$X_{11} = 0$|10  |1|
  |$X_{11} = 1$|1   |10|

```py
from pgmpy.factors.discrete import DiscreteFactor

factor = DiscreteFactor(['X11', 'X12'], cardinality=[2, 2], 
                        values=[[10, 1], [1, 10]])
model.add_factors(factor)
```

