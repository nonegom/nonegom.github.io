---
title:  "[머신러닝] 지도학습 - 4.4. 퍼셉트론"
excerpt: "판별함수기반 분류모형 중 하나인 퍼셉트론 모형"

categories:
  - MachinLearning
tags:
  - SupervisedLearning
  - 10월
toc: true
toc_sticky: true
toc_label: 페이지 목차
use_math: true
---
> 아래 포스팅은 기존 수업의 복습 차원에서 올리는 포스팅입니다. 따라서 세부적인 수학적 정리가 생략된 부분이 있습니다. 따라서 좀 더 구체적인 정보나 원하시면 [데이터 사이언스 스쿨 사이트](https://datascienceschool.net/)를 참고 부탁드립니다. 특히 아래 코드를 이용한 시각화 그래프 코드와 모습을 보고 싶으시면 링크 확인부탁드립니다.  

## 0. 퍼셉트론
 가장 오래되고 단순한 형태의 판별함수기반 분류모형 중 하나로 딥러닝 분야에도 활용된다.
 
- 이제부터 나오는 퍼셉트론은 1또는 -1의 값을 가지는 y를 출력한다.
- 1을 포함하는 입력 요소 $x_i$에 대해 가중치 $w_i$를 곱한 값 $a=w^Tx$을 활성화값(activations)이라고 하며 이 값이 **판별함수의 역할**을 한다.
- 판별 함수 값이 활성화함수 $h(a)$를 지나면 분류 결과를 나타내는 출력 $\hat{y}$가 생성된다.
$$\hat{y} = h(w^Tx)$$ 
- 퍼셉트론의 활성화 함수는 **부호함수** 또는 **단위계단함수**라고 부르는 함수이다. (활성화값이 음수이면 -1, 양수이면 1)


## 1. 퍼셉트론 손실함수

퍼셉트론은 독립변수 x로부터 종속변수 y를 예측하는 예측 모형이므로 모든 학습 데이터에 대해 예측 오차를 최소화하는 가중치 $w$를 계산해야 한다. 가중치 $w$에 따라 달라지는 전체 예측오차 $L$은 i번째 개별 데이터에 대한 손실함수 $L_i(\hat{y}_i,y_i)$의 합으로 표현할 수 있다.
$$L = \sum^N_{i=1}L_i(y_i,\hat{y}_i)$$


이전 회귀분석에서는 '오차제곱합'을 줄이고자 했지만,여기서는 **제로-원 손실함수**(zer0-one loss function)라는 것을 이용한다.
$$L_i(y, \hat{y}) = max(0, -y\hat{y})$$
-**제로-원 손실함수** $L_i$(개별적 손실함수)는 $\hat{y}$과 $y$가 같으면 0이고, 다르면 1이다.  


전체 손실함수는 **오분류된 데이터의 집합($M$)의 합**으로만 나타낼 수도 있다.
$$L_i(y, \hat{y}) = max(0, -y\hat{y}) = - \sum_{i \in M}y_i \hat{y}$$
- 여기서 $\sum$ 앞에 '-'를 해주는 이유는 '-1'인 값의 애들만 더해주기 때문이다. 이 식에서 $M$은 오분류된 데이터의 집합이다. $y$와 $\hat{y}$값이 다르면 오분류된 것이다.


그런데 '제로-원 손실함수'를 쓰면 $\hat{y}(x)$가 x에 대해 계단형 함수이기에 대부분의 영역에서 **기울기가 0이 되어 미분값으로부터 최소점의 위치(수치적 최적화)를 구할 수 없다.** 따라서 $\hat{y}$대신에 **활성화값(activation) $w^Tx$을 그대로 손실함수**로 쓴다. 이를 **퍼셉트론 손실함수(perceptron loss function)** 또는 **0-힌지 손실함수**라고 한다. 
$$L_p(w) = - \sum_{i \in M} y_i \cdot w^Tx_i$$

- 판별 경계선은 직선이지만, 모양에서는 비선형이다. 
- 이때 손실값은 오분류된 표본에 대해서만 계산한다. $y$와 $sng(\hat{y})$의 값이 다르면 오분류된 것이다.

### 가중치 계산

퍼셉트론 손실함수 $L_p(w)$를 최소화하는 $w$를 찾기 위해 $L_p(w)$를 $w$로 미분하여 그레디언트를 구하면 다음과 같다.
$$\frac{d L_p}{dw} = - \sum_{i \in M} x_i y_i $$

**그레디언트 디센트**(gradient descent) 방법을 사용하면 다음과 같이 $w$를 갱신할 수 있다.
$$w_{k+1} = w_k + \eta_k \sum_{i \in M} x_iy_i$$

여기에서 **$\eta$**는 **스텝사이즈(stepsize)** 또는 **학습속도(learning rate)**라고 한다. 실제로는 계산량을 줄이기 위해 오분류 데이터 집합 $M$ 중에서 무작위로 고른 하나의 데이터 $m$을 통해 계산을 한다.
$$w_{k+1} = w_k + \eta_k x_m y_m$$
- 그런데 퍼셉트론 손실함수는 원래의 손실함수와 정확하게 같지 않다. 따라서 매 단계마다 반드시 원래의 손실함수가 감소한다는 보장은 없다. 다만 **퍼셉트론 수렴 정리**로부터 데이터가 선형분리 가능한 경우에는 완전분류모형으로 수렴한다는 것이 증명되어 있다.

### Scikit-Learn의 퍼셉트론 구현
Scikit-Learn에서 제공하는 퍼셉트론 모형인 `perceptron`클래스는 다음과 같은 **입력 인수**를 가진다.
- `max_iter` : 최적화를 위한 반복 횟수(iteration number)
- `eta0` : 학습속도
- `n_iter_no_change` : 이 설정값만큼 반복을 해도 성능이 나아지지 않으면 max_iter 설정값과 상관없이 멈춘다.

```py
## 0. 데이터 생성
from sklearn.datasets import load_iris
iris = load_iris()
idx = np.in1d(iris.target, [0, 2])
X = iris.data[idx, :2]
y = (iris.target[idx] / 2).astype(np.int)


## 1. 모델 생성
from sklearn.linear_model import Perceptron
from sklearn.metrics import confusion_matrix, classification_report

model = Perceptron(max_iter=400, shuffle=False, tol=0, n_iter_no_change=1e9).fit(X, y)
confusion_matrix(y, model.predict(X))

## 2. 퍼셉트론 그래프 
n = 400
loss = np.zeros(n)
model = Perceptron(warm_start=True, shuffle=False)
for i in range(n):
    model.partial_fit(X, y, classes=[0, 1])
    loss[i] = np.sum(y != model.predict(X))
plt.plot(loss)
plt.xlabel("횟수")
plt.title("오분류된 데이터의 갯수")
plt.show()
```
![](/assets/images/Supervised9_1.png)

## 2. SGD 
SGD(Stochastic Gradient Descent) 방법은 손실함수 자체가 아니라 **손실함수의 기댓값을 최소화**하는 방법이다. SGD최적화 방법은 그레디언트가 아니라 **그레디언트의 기댓값의 추정치**를 이용한다.
$$w_{k+1} = w_k + E[ \triangledown L]$$

그레디언트의 기댓값의 추정치는 **'표본평균'**이다. 즉, **미니배치(minibatch)**라고 부르는 일부의 데이터만을 사용해 그레디언트 추정치를 구한다. **퍼셉트론**은 오분류(miss-classified)된 데이터만 이용하는 SGD의 일종이다.
- 한 번의 계산량이 많거나, 학습 데이터가 많은 딥러닝에 사용된다. 

> SGD방법에서 기댓값이 최소화되도록 수렴하는 것은 맞지만, **손실함수의 기댓값의 추정치**를 최대화하기 때문에 손실함수값이 전반적으로 **감소하는 추세**를 보이는 것 뿐, 항상 절대적으로 **감소한다는 보장은 없다**.

### sklearn의 SGD구현

 SGD에서는 '제로-원'이나 '퍼셉트론 손실함수' 이외에도 손실함수가 볼록함수(convex function)이면 모두 개별 데이터 손실함수로 사용할 수 있다. 아래에는 'y=1인 경우 많이 사용되는 손실함수의 값'을 나타내었다.
 ![](/assets/images/Supervised9_2.png)

`SGDClassifier 클래스`
- Perceptron 클래스의 입력 인수 이외에도 손실함수를 결정하는 `loss`인수를 가진다. 
- loss인수의 가능한 값
    - hinge, perceptron, log, huber, modified_huber, squared_hindge 등이다.
    - 보통 midied_huber(수정 휴버)손실 함수를 사용하거나 squared_hindge(제곱힌지)손실함수를 사용한다.

```py
## 0. 데이터 생성
  # 위 코드와 동일

## 1. 모델 생성
model = SGDClassifier(loss="modified_huber", max_iter=400, shuffle=False, n_iter_no_change=1e9).fit(X, y)
confusion_matrix(y, model.predict(X))

## 2. 퍼셉트론 그래프
n = 400
loss = np.empty(n)
model = SGDClassifier(loss="modified_huber", shuffle=False)
for i in range(n):
    model.partial_fit(X, y, classes=[0, 1])
    loss[i] = np.sum(y != model.predict(X))
plt.plot(loss)
plt.xlabel("횟수")
plt.title("오분류된 데이터의 갯수")
plt.show()
```
![](/assets/images/Supervised9_3.png)