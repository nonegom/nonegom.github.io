---
title:  "[머신러닝] 지도학습 - 1.4. 분류 성능평가"
excerpt: "다양한 분류 성능평가 기준 및 점수 / 분류 결과표(confusion_matrix), classification_report, roc_curve"

categories:
  - ML_Supervised
tags:
  - SupervisedLearning
  - 10월
toc: true
toc_sticky: true
toc_label: 페이지 목차
use_math: true
---
> 아래 포스팅은 기존 수업의 복습 차원에서 올리는 포스팅입니다. 좀 더 구체적인 자료를 원하시면 [데이터 사이언스 스쿨 사이트](https://datascienceschool.net/03%20machine%20learning/09.04%20%EB%B6%84%EB%A5%98%20%EC%84%B1%EB%8A%A5%ED%8F%89%EA%B0%80.html) 참고 부탁드립니다. 특히 아래 코드의 plt 그래프의 모습을 보고 싶으시면 확인부탁드립니다.

## 0. 분류 성능 평가
 분류 문제는 회귀 분석과 달리 다양한 성능평가 기준이 필요하다. 이 절에서는 분류문제에 사용되는 다양한 성능 평가 기준에 대해 알아본다.

## 1. 사이킷런 패키지에서 지원하는 분류 성능평가 명령
 사이킷런 패키지는 metrics 서브패키지에서 다양한 분류용 성능평가 명령을 제공한다.

- confusion_matrix(y_true, y_pred)
- accuracy_score(y_true, y_pred)
- precision_score(y_true, y_pred)
- recall_score(y_true, y_pred)
- fbeta_score(y_true, y_pred, beta)
- f1_score(y_true, y_pred)
- classfication_report(y_true, y_pred)
- roc_curve
- auc

## 2. Confusion Matrix (분류결과표) 
 타켓의 원래 클래스와 모형이 예측한 클래스가 일치하는지 갯수로 센 결과를 표로 나타낸 것. 정답 클래스는 행(row) 예측한 클래스는 열(column)로 나타낸다.
 `confusion_matrix`

```py
from sklearn.metrics import confusion_matrix
# 정답인 y값과 분류모형이 예측한 y값 가정
y_true = [2, 0, 2, 2, 0, 1]
y_pred = [0, 0, 2, 2, 0, 2]

confusion_matrix(y_true, y_pred)

# 결과 값
array([
[2, 0, 0],
[0, 0, 1],
[1, 0, 2]])
```

- 표에서 대각선 내의 개수가 많으면 예측이 잘된 것, 비대각 성분에 개수가 많으면 예측이 잘 안된 것

## 3. 이진 분류결과표 (Binary Confusion Matrix) ★ 
일반적으로 클래스 이름을 Negative(음성), Positive(양성)으로 표시한다. 이때의 결과를 나타내는 이진 분류 결과표는 다음과 같다.

|         | 양성이라고 예측 | 음성이라고 예측|
|----|----:|----:|
|실제 양성 | True Positive(TP) | False Negative (FN) |
|실제 음성 | False Positive(FP) | True Negative (TN)|

아래 예시를 통해 좀 더 알아보도록 하겠다.

### 이진분류 시스템의 예
이진분류 시스템으로는 아래와 같은 예가 있지만, 그 중 '제조공장에서의 불량품 찾기' 예제를 통해 설명해보겠다.
1. 제조공장에서 불량품 찾기
2. 암을 검진함에 있어 양성, 음성 구분
3. FDS(Fraud Detection System) 금융거래 등에서 사기거래 찾아내는 시스템

> 해당 예시에 대한 더 구체적인 설명은 위 [링크](https://datascienceschool.net/03%20machine%20learning/09.04%20%EB%B6%84%EB%A5%98%20%EC%84%B1%EB%8A%A5%ED%8F%89%EA%B0%80.html)를 참조

### '제조공장에서의 불량품 찾기'
제품을 생산하는 제조공장에서는 완성된 제품에 대해 품질 테스트를 실시하여 불량품을 찾아내고 찾아낸 불량품은 공장으로 리콜(recall)시킨다. 이 때 품질 테스트 결과가 **양성이면 불량품**이라고 예측한 것이고 **음성이면 정상제품**이라고 예측한 것이다.

- **True Positive**: 불량품을 불량품이라고 정확하게 예측
- **True Negative**: 정상제품을 정상제품이라고 정확하게 예측
- False Positive: 정상제품을 불량품이라고 잘못 예측
- False Negative: 불량품을 정상제품이라고 잘못 예측


    |         | 불량품이라고 예측 | 정상제품이라고 예측|
    |----|----:|----:|
    |실제 불량품 | True Positive(TP) | False Negative (FN) |
    |실제 정상제품 | False Positive(FP) | True Negative (TN)|


- 이 표를 볼 때는 보통 예측을 정확하게 맞춘 **'TP'**와 **'TN'**에 집중을 하면 된다.

- 그런데 만약 제품의 경우 'FP'와 'FN'이 가지는 무게감은 다르다. 만약 **정상제품을 불량품(FP)**이라고 했다면 조금 시간이나 재화가 소비될 뿐이지만, 만약 **불량품을 정상제품(FN)**이라고 했을 경우 불량품이 소비자에게 나가게 돼서 문제가 커지게 된다.

> `confusion_matrix`명령을 사용할 때는 클래스 순서가 0, 1, 2 ...순서로 출력되기 때문에 위에서 표시한 표와 다를 수 있으므로 주의해야 한다. 그래서 `labels` 인수를 사용해서 순서를 바꿀 수 있다.

```py
y_true = [1, 0, 1, 1, 0, 1]
y_pred = [0, 0, 1, 1, 0, 1]
confusion_matrix(y_true, y_pred)

# labels인수 사용해서 '이진분류 결과표'모양으로 만든다.
confusion_matrix(y_true, y_pred, labels=[1, 0])
```

## 4. 평가점수
이진 분류평가표로부터 하나의 평가점수(score)를 계산하여 그 값을 최종적인 기준으로 사용하는 경우가 많은데, 관점에 따라 다양한 평가점수가 쓰인다.

### 1. 정확도 (accuracy)
 전체 샘플 중 맞게 예측한 샘플 수의 비율을 뜻한다. 높을수록 좋은 모형.
$$ accuracy = \frac{TP+TN}{TP+TN+FP+FN} $$

### 2. 정밀도 (precision)
 '양성' 클래스에 속한다고 출력한 샘플 중 실제로 '양성' 클래스에 속하는 샘플 수의 비율을 뜻한다. 높을 수록 좋은 모형
- 제조공장에서 **불량품이라고 판단**한 것들 중 실제 불량품의 비율
$$ precision = {TP \over TP+FP} $$

### 3. 재현율 (recall) / TPR
 실제 양성 클래스에 속한 표본 중 양성 클래스에 속한다고 출력한 표본 수의 비율을 뜻한다. 높을 수록 좋은 모형
- **진짜 불량품** 중에서 잡혀들어간 불량품의 비율 **(정의구현율)**
- **TPR(true positive rate)** 또는 민감도(sensitivity)라고도 한다.
$$reall = {TP \over TP+FN}$$

### 4. 위양성율 (fall-out) / FPR
- 실제 양성 클래스에 속하지 않는 표본 중 양성 클래스에 속한다고 출력한 표본의 비율. 다른 평가점수와 다르게 **낮을 수록 좋은 모형**
- 제조공장에서 실제는 정상제품인데, 불량품이라고 예측된 제품의 비율이다. 즉, 억울한 제품들(사람들)이 걸린 비율
- **FPR(false positive rate)**또는 1에서 위양성률의 값을 뺀 값을 특이도
(specificity)라고도 한다.
$$fall-out = {FP \over FP+TN}$$

### 5. F 점수
- 정밀도와 재현율의 가중조화평균 (weight harmonic average)
- 정밀도에 주어지는 가중치를 $\beta$(베타)라고한다.
$$F_\beta = (1+\beta^2)(precision \times reacall)\ /\ (\beta^2precison+recall)$$

- 베타가 1인 경우를 특별히 **'F1점수'**라고 한다.
$$F_1 = 2\cdot(precision \cdot reacall)\ /\ (precison+recall)$$

### classification_report
 위 구현점수를 모두 구해주는 `classification_report`가 있다.
 ```py
from sklearn.metrics import classification_report
# 샘플 데이터
y_true = [0, 0, 0, 1, 1, 0, 0]
y_pred = [0, 0, 0, 0, 1, 1, 1]

print(classification_report(y_true, y_pred, target_names=['class 0', 'class 1']))
 ```
![]({{site.url}}/assets/images/guidance1.jpg){: .align-center}

#### classifiaction_report 설명
- precision: 실제 클래스에서 정확하게 예측한 비율
- recall: 진짜 0중에서 잡힌 0의 비율
- f1-score: precision과 recall의 중간값이 나온다. 
- support: 실제 클래스의 개수
- accuracy: 정확하게 맞춘 클래스들의 비율
- macro avg: 그냥 평균
- weighted avg: support의 가중치를 주어서 계산한 평균

## 5. ROC커브(Receiver Operator Characteristic Curve) 

**재현율과 위양성률은 일반적으로 양의 상관 관계가 있다.**  

**'재현율'**을 높이기 위해서는 양성으로 판단하는 기준을 낮추어 약간의 증거만 있어도 양성으로 판단하게 하면 된다. 그러나 이렇게 되면 음성임에도 양성으로 판단되는 표본 데이터가 같이 증가하게 되어 **'위양성율'**이 동시에 증가한다.

![출처: 데이터 사이언스 스쿨]({{site.url}}/assets/images/guidance2.jpg){: .align-center}
 <center>클래스 판별 기준값의 변화에 따른 '위양성률'과 '재현율' 시각화</center>

- **ROC커브**는 클래스 판별 **기준값(threshold)의 변화**에 따른 위양성률과 재현율의 변화를 시각화 한것이다.
 정상적인 경우 재현율(recall)이 위양성율(fall-out)보다 높다. 그런데 정상인 경우 중 최악은 recall과 fall-out의 수치가 5:5인 경우. 즉, 직선 그래프인 경우(random shuffle)

 ![출처: 데이터 사이언스 스쿨]({{site.url}}/assets/images/guidance3.jpg){: .align-center}


### ROC커브 작성법

1. 현재는 0을 기준값(threshold)으로 클래스를 구분하여 판별함수값이 0보다 크면 양성(Positive), 작으면 음성(negative)이 된다.
2. 데이터 분류가 다르게 되도록 기준값을 증가 혹은 감소시킨다.
3. 기준값을 여러가지 방법으로 증가 혹은 감소시키면서 이를 반복하면 여러가지 다른 기준값에 대해 분류 결과가 달라지고 "재현율", "위양성률" 등의 성능평가 점수도 달라진다.

- 대부분의 모형에서는 'accuracy'가 높은 게 중요하지만 'recall'을 높게 형성해야 하는 경우 ROC커브를 그려본다.
- ROC커브는 좌상단에 가까운 게 더 성능이 좋은 것이다. 

- **`roc_curve`**
- 인수: 'target y 벡터'와 '판별함수 벡터(혹은 확률 벡터)'
- 결과 (반환값): 기준값을 사용했을 때의 '재현율'과 '위양성률' 그리고 '변화되는 기준값(threshold)' 을 반환한다.

### ROC커브 예제 코드
```py
# 가상데이터 생성
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
X, y = make_classification(n_samples=16, n_features=2,
n_informative=2, n_redundant=0, random_state=0)

# 모델 생성(로지스틱 회귀모형)
model = LogisticRegression().fit(X, y)
y_hat = model.predict(X)

# 회귀모형을 사용해서 판별함수 값 구함
f_value = model.decision_function(X)

# roc_curve 사용
from sklearn.metrics import roc_curve
fpr, tpr, thresholds = roc_curve(y, model.decision_function(X))
fpr, tpr, thresholds

# 만약 decision_function 미 지원시, 
## predict_proba 명령을 써서 확률입력 가능 (위 코드와 fpr, tpr 값 비슷)
fpr, tpr, thresholds = roc_curve(y, model.predict_proba(X)[:, 1])
fpr, tpr, thresholds
```

### 6. AUC (Area Under the Curve)
 **ROC Curve의 면적**을 뜻한다. 이를 통해 성능을 비교할 수 있다. 위양성률(fall-out / FPR)의 값이 작을수록, AUC가 1에 가까운 값이고 좋은 모형이다. 
 ```py
from sklearn.metrics import auc
auc(fpr, tpr),
 ```

### 다중 클래스의 ROC커브
 다중 클래스에서는 ROC 커브를 그릴 수 없다.따라서 각각의 클래스에 대해 **OvR문제**를 풀고있다고 가정하고, ROC 커브를 그린다.