---
title:  "[머신러닝] 지도학습 - 6.4. 대규모 데이터 학습"
excerpt: "실제 양적으로 데이터가 많은 경우 데이터를 분석하는 방법"

categories:
  - MachinLearning
tags:
  - SupervisedLearning
  - 10월
toc: true
toc_sticky: true
toc_label: 페이지 목차
use_math: true
---
> 아래 포스팅은 기존 수업의 복습 차원에서 올리는 포스팅입니다. 따라서 세부적인 수학적 정리가 생략된 부분이 있습니다. 따라서 좀 더 구체적인 정보나 원하시면 [데이터 사이언스 스쿨 사이트](https://datascienceschool.net/03%20machine%20learning/14.04%20%EB%8C%80%EA%B7%9C%EB%AA%A8%20%EB%8D%B0%EC%9D%B4%ED%84%B0%20%ED%95%99%EC%8A%B5.html)를 참고 부탁드립니다. 특히 아래 코드를 이용한 시각화 그래프 코드와 모습을 보고 싶으시면 링크 확인부탁드립니다.  

## 0. 대규모 데이터 학습
**대규모 데이터(BigData)**의 경우에는 메모리 등의 문제로 특정한 모형은 사용할 수 없는 경우가 많다. (여기서는 `selection`의 방법을 사용하지 않는다)이 때에는 **대용량의 데이터를 소화**할 수 있는 모델과 소화할 수 없는 모델(서포트 벡터머신)을 구분해 사용한다.  
아래와 같은 모형들을 이용하고, 전체 데이터를 처리 가능한 작은 조각으로 나누어 학습을 시키는 **점진적 학습 방법**을 사용한다.

- **사전 확률분포**를 설정할 수 있는 생성 모형
- **시작 가중치**를 설정할 수 있는 모형


- 해당 방법론들은 회귀분석 문제를 푸는데도 사용될 수 있다.

```py
from sklearn.datasets import fetch_covtype
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

# covtpye데이터 사용(sklearn 데이터 중 많은 편)
covtype = fetch_covtype(shuffle=True, random_state=0)
X_covtype = covtype.data
y_covtype = covtype.target - 1
classes = np.unique(y_covtype)
X_train, X_test, y_train, y_test = train_test_split(X_covtype, y_covtype)
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 실무에서는 파일이나 데이터베이스에서 읽어온다.
# 데이터를 메모리에 한 번에 올릴 수 없다고 가정해서 파일의 일부만 가져오는 함수
def read_Xy(start, end):
  idx = list(range(start, min(len(y_train) - 1, end)))
  X = X_train[idx, :]
  y = y_train[idx]
  return X, y
```

## 1. SGD
퍼셉트론 모형은 가중치를 계속 업데이트하므로 **일부 데이터를 사용하여 구한 가중치**를 **다음 단계에서 초기 가중치로 사용**할 수 있다.
- 즉, 내가 데이터를 다 집어넣어도, 일부만 랜덤하게 사용해서 **그레디언트 벡터**로 사용한다. 따라서 데이터를 일부러 일부만 집어넣는다. 대신 다음 번에 할 때는 또 다른 데이터를 집어넣어줘야 한다.

- 데이터를 모두 한 번씩 다 넣어서 사이클 한 번이 도는 경우를 'epoch'라고 한다.

```py
from sklearn.linear_model import SGDClassifier
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score

model = SGDClassifier(random_state=0)
n_split = 10
n_X = len(y_train) // n_split
n_epoch = 10
for epoch in range(n_epoch): # 총 10회 반복
    for n in range(n_split):
        X, y = read_Xy(n * n_X, (n + 1) * n_X)
        model.partial_fit(X, y, classes=classes)
    # accuracy를 할 때에는 **데이터 전체**에 대해서 해야 한다.
    accuracy_train = accuracy_score(y_train, model.predict(X_train))
    accuracy_test = accuracy_score(y_test, model.predict(X_test))
    print("epoch={:d} train acc={:5.3f} test acc={:5.3f}".format(epoch, accuracy_train, accuracy_test))

"""> 출력값 
epoch=0 train acc=0.707 test acc=0.707
epoch=1 train acc=0.710 test acc=0.710
...
epoch=9 train acc=0.711 test acc=0.711

epoch가 증가할 때마다 정확도가 올라가지만 매번 증가하진 않는다.
"""
```

## 2. 나이브 베이즈 모형
생성모형은 일부 데이터를 이용해 구한 확률분포를 '사전확률분포'로 사용할 수 있다. 
- 대표 수종 데이터 중 X11부터 0과 1로 이루어진 데이터므로, **베르누이 나이브 베이즈 모형**을 사용할 수 있다.

```py
from sklearn.naive_bayes import BernoulliNB
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score

model = BernoulliNB(alpha=0.1)
n_split = 10
n_X = len(y_train) // n_split

for n in range(n_split):
X, y = read_Xy(n * n_X, (n + 1) * n_X)
  model.partial_fit(X, y, classes=classes)
  accuracy_train = accuracy_score(y_train, model.predict(X_train))
  accuracy_test = accuracy_score(y_test, model.predict(X_test))
  print("n={:d} train accuracy={:5.3f} test accuracy={:5.3f}".format(n, accuracy_train, accuracy_test))

"""> 출력값 
n=0 train accuracy=0.630 test accuracy=0.628
n=1 train accuracy=0.630 test accuracy=0.629
n=2 train accuracy=0.632 test accuracy=0.630
n=3 train accuracy=0.633 test accuracy=0.632
n=4 train accuracy=0.633 test accuracy=0.631
...
n=9 train accuracy=0.632 test accuracy=0.630

epoch가 증가할 때마다 정확도가 올라가지만 매번 증가하진 않는다.
"""
```
## 3. 그레디언트 부스팅
초기 **커미티 멤버**로 일부 데이터를 사용해 학습한 모형을 사용할 수 있다.

```py
from lightgbm import train, Dataset
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score

params = {
    'objective': 'multiclass',
    "num_class": len(classes),
    'learning_rate': 0.2,
    'seed': 0,
}

n_split = 10
n_X = len(y_train) // n_split
num_tree = 10
model = None
for n in range(n_split):
    X, y = read_Xy(n * n_X, (n + 1) * n_X)
    model = train(params, init_model=model, train_set=Dataset(X, y),
                  keep_training_booster=False, num_boost_round=num_tree)
    accuracy_train = accuracy_score(y_train, np.argmax(model.predict(X_train), axis=1))
    accuracy_test = accuracy_score(y_test, np.argmax(model.predict(X_test), axis=1)) 
    print("n={:d} train accuracy={:5.3f} test accuracy={:5.3f}"
                .format(n, accuracy_train, accuracy_test))

"""> 출력값 
n=0 train accuracy=0.769 test accuracy=0.767
n=1 train accuracy=0.792 test accuracy=0.788
n=2 train accuracy=0.806 test accuracy=0.802
n=3 train accuracy=0.816 test accuracy=0.812
...
n=8 train accuracy=0.795 test accuracy=0.790
n=9 train accuracy=0.794 test accuracy=0.789

epoch가 증가할 때마다 정확도가 올라가지만 매번 증가하진 않는다.
"""
```

## 4. 랜덤 포레스트
랜덤 포레스트와 같은 **앙상블 모형**에서는 일부 데이터를 사용한 모형을 **개별 분류기**로 사용할 수 있다. 랜덤 포레스트에서는 `warm_stautus =True`로 주면, `partial_fit`을 할 때마다 안의 트리의 개수를 
늘릴 수 있다. 
- 위와 같은 방법은 Gradient Boosting방법에서도 유사하게 사용할 수 있다.

```py
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score

n_split = 10
n_X = len(y_train) // n_split
num_tree_ini = 10
num_tree_step = 10
# 기존의 트리는 건들지 않고, 새로운 트리만 분류한다.
model = RandomForestClassifier(n_estimators=num_tree_ini, warm_start=True)
for n in range(n_split):
    X, y = read_Xy(n * n_X, (n + 1) * n_X)
    model.fit(X, y)
    accuracy_train = accuracy_score(y_train, model.predict(X_train))
    accuracy_test = accuracy_score(y_test, model.predict(X_test))
    print("epoch={:d} train accuracy={:5.3f} test accuracy={:5.3f}"
                .format(n, accuracy_train, accuracy_test))
    # 단계가 올라갈 때마다 나무의 개수가 증가한다.
    model.n_estimators += num_tree_step
"""
epoch=0 train accuracy=0.868 test accuracy=0.855
epoch=1 train accuracy=0.892 test accuracy=0.874
...
epoch=8 train accuracy=0.907 test accuracy=0.888
epoch=9 train accuracy=0.907 test accuracy=0.888
"""
```