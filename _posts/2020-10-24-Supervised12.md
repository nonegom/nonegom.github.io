---
title:  "[머신러닝] 지도학습 - 5.3. 커널 서포트 벡터 머신"
excerpt: "XOR문제를 풀기 위해 커널을 사용한 커널 서포트 벡터머신"

categories:
  - MachinLearning
tags:
  - SupervisedLearning
  - 10월
toc: true
toc_sticky: true
toc_label: 페이지 목차
use_math: true
---
> 아래 포스팅은 기존 수업의 복습 차원에서 올리는 포스팅입니다. 따라서 세부적인 수학적 정리가 생략된 부분이 있습니다. 따라서 좀 더 구체적인 정보나 원하시면 [데이터 사이언스 스쿨 사이트](https://datascienceschool.net/03%20machine%20learning/13.03%20%EC%BB%A4%EB%84%90%20%EC%84%9C%ED%8F%AC%ED%8A%B8%20%EB%B2%A1%ED%84%B0%20%EB%A8%B8%EC%8B%A0.html)를 참고 부탁드립니다. 특히 아래 코드를 이용한 시각화 그래프 코드와 모습을 보고 싶으시면 링크 확인부탁드립니다.  

## 0. XOR 문제
퍼셉트론이나 서포트 벡터 머신과 같은 선형판별함수 분류모형은 다음과 같은 **XOR(exclusive OR) 문제**를 풀지 못한다는 단점이 있다.

|   |X_2 = 0|X_2 = 1|
|:---:|---:|---:|
|X_1 = 0|  0|  1|
|X_1 = 1|  1|  0|

위 표의 경우처럼 데이터가 분리되어 있는 경우, **선형 판별평면 (decision hyperplane)**으로 영역을 나눌 수 없기 때문이다.
```py
### 데이터 생성

np.random.seed(0)
X_xor = np.random.randn(200, 2)
y_xor = np.logical_xor(X_xor[:, 0] > 0, X_xor[:, 1] > 0)
y_xor = np.where(y_xor, 1, 0)
plt.scatter(X_xor[y_xor == 1, 0], X_xor[y_xor == 1, 1],
c='b', marker='o', label='클래스 1', s=50)
plt.scatter(X_xor[y_xor == 0, 0], X_xor[y_xor == 0, 1],
c='r', marker='s', label='클래스 0', s=50)
plt.legend()
plt.xlabel("x_1")
plt.ylabel("x_2")
plt.title("XOR 문제")
plt.show()
```
![](/assets/images/Supervised12_0.JPG)
![](/assets/images/Supervised12_1.JPG)


## 1. 기저함수를 사용한 비선형 판별 모형
지금까지 '선형 판별모형' $f = w^Tx$ 를 사용했다. 그런데 '비선형 판별모형' $f = x^TAx+w^Tx$ 같은 이차형식을 선형판별모형으로 만들기 힘들다.
따라서 **'선형 판별모형'**을 사용하되 기존 D차원 독립변수 벡터 $x$ 대신에 $\phi$라는 **x에 대해 '비선형 함수'를 적용**한 M차원 독립변수 벡터 $\phi(x)$를 만들어 사용한다. 그렇게 하면 '선형판별모형' $w^T\phi(x)$ 이 된다. 이렇게 했을 경우 $w$ 계산은 똑같이 해도 **$x$의 판별함수는 곡선**이 된다.
$$\phi(\cdot): R^D → R^M$$
$$x = (x_1, x_2, \cdots, X_D) \rightarrow \phi(x) = (\phi_1(x), \phi_2(x), \cdots, \phi_M(x))$$
앞서 XOR문제를 풀기 위해 다음과 같이 상호 곱(cross-multiplication)항을 추가한 기저함수를 사용해보자.
$$(x_1, x_2) \rightarrow \phi(x) = (x_1^2, \sqrt{2}x_1x_2, x_2^2)$$

### 예제 코드
`FunctionTransformer` 전처리 클래스로 기저함수를 이용한 특징 변환을 할 수 있다. 
```py
# 샘플 데이터
X = np.arange(6).reshape(3, 2)
"""
array([[0, 1],
       [2, 3],
       [4, 5]])
"""

## 기저함수를 이용한 특징 변환
from sklearn.preprocessing import FunctionTransformer
def basis(X):
  return np.vstack([X[:, 0]**2, np.sqrt(2)*X[:, 0]*X[:, 1], X[:, 1]**2]).T

FunctionTransformer(basis).fit_transform(X)
"""
array([[ 0. , 0. , 1. ],
      [ 4. , 8.48528137, 9. ],
      [16. , 28.28427125, 25. ]])
"""
def plot_xor(X, y, model, title, xmin=-3, xmax=3, ymin=-3, ymax=3):

    #### plot 그리는 함수 생략 (출처 사이트 참고) ###
```
위와 같은 기저함수를 써서 XOR 문제의 데이터를 변환하면 특성 $\phi_2$를 사용하여 클래스 분류를 할 수 있다는 것을 알 수 있다.  
하지만 보통 사람이 '기저함수 ($\phi)'값을 알아내야 한다. 따라서 커널 서포트 벡터머신을 더 발전시킨게 바로 **딥러닝**이다. 딥러닝을 하면 사람이 아닌 컴퓨터가 대신해서 **'비선형 기저 함수'** 값을 찾아준다.

```py
from sklearn.pipeline import Pipeline

# 변환을 하기 제대로 위해 Pipeline을 사용한다.  
basismodel = Pipeline([("basis", FunctionTransformer(basis)),
                      ("svc", SVC(kernel="linear"))]).fit(X_xor, y_xor)
plot_xor(X_xor, y_xor, basismodel, "기저함수 SVC 모형을 사용한 XOR 분류 결과")
plt.show()
```
![](/assets/images/Supervised12_1.png)

## 2. 커널 트릭
서포트 벡터 머신의 경우 목적 함수와 예측 모형은 다음과 같은 dual form으로 표현할 수 있다.
$$L = \sum_{n=1}^Na_n - \frac{1}{2} \sum_{n=1}^N\sum_{m=1}^Na_na_my_ny_mx_n^Tx_m$$
$$y = w^Tx - w_0 = \sum^N_{n=1}a_ny_nx_n^Tx-w_0$$

위 수식에서 **$x$**를 기저함수 변환으로 **$\phi(x)$**로 바꾸면 아래와 같은 식이 된다.
$$L = \sum_{n=1}^Na_n - \frac{1}{2} \sum_{n=1}^N\sum_{m=1}^Na_na_my_ny_m\phi(x)_n^T\phi(x)_m$$
$$y = w^Tx - w_0 = \sum^N_{n=1}a_ny_n\phi(x)_n^T\phi(x)-w_0$$

위의 기저함수를 보면 항상 $\phi(x_i)_n^T\phi(x_j)$의 형태로만 사용되며 독립적으로 사용되지 않는다. 따라서 **두 개의 독립변수 벡터를 내적**한 값 $\phi(x_i)_n^T\phi(x_j)$을 하나의 함수로 나타낼 수 있다.
$$k(x_i, x_j) = \phi(x_i)_n^T\phi(x_j)$$
여기서 k(x_i,x_j)와 같은 함수를 커널(kernel)이라고 한다.
- 커널함수는 **벡터가 2개** 들어가서 **스칼라 값**이 나온다.
- 만약 대응하는 기저함수가 존재할 수 있다면, 커널을 먼저 정의해도 상관이 없다.(기저함수를 먼저 정의하고, 다음에 커널을 정의할 필요가 없다. 이렇게 하면 '기저함수'를 만드는 문제에서 '커널함수'를 만드는 문제로 바뀌게 된다.)

### 커널의 의미
서포트 벡터머신의 목적함수와 예측모형은 커널을 사용해 표현하면 아래와 같이 바꿀 수 있다.
$$L = \sum_{n=1}^Na_n - \frac{1}{2} \sum_{n=1}^N\sum_{m=1}^Na_na_my_ny_k(x_n,x_m)$$
$$y = w^Tx - w_0 = \sum^N_{n=1}a_ny_nk(x_n,x)-w_0$$

커널을 사용하지 않는 경우 $k(x, y) = x^Ty$라는 점을 고려하면 다음과 같은 특징을 보인다.
- $x$와 $y$가 동일한 벡터일 때 가장 크다.
- $x$와 $y$두 벡터간의 거리가 멀어질 수록 작아진다.

### 커널 사용의 장점
커널을 사용하면 베이시스 함수를 하나씩 정의하는 수고를 덜 수 있을 뿐만 아니라 변환가 내적에 들어가는 계산량이 줄어들게 된다. 커널방법을 쓰지 않을 경우 $\phi(x_1)_n^T\phi(x_2)$를 연산하기 위해 11번의 곱을 해야 한다.
- $\phi(x_1)$ 계산: 곱셈 4회
- $\phi(x_2)$ 계산: 곱셈 4회
- 내적: 곱셈 3회

 커널을 사용하면 \phi(x_1)_n^T\phi(x_2)$$을 계산하는데 3번의 곱셈이면 된다.
- $\phi(x_1)$^T$\phi(x_2)$: 곱셈 2회
- 내적: 곱셈 1회

### 커널의 확장 생성
다음 규칙을 이요하면 이미 만들어진 커널 $k_1(x_1, x_2), k_2(x_1, x_2)$로부터 새로운 커널을 쉽게 만들 수 있다.

1. 커널함수를 양수배한 함수는 커널함수이다.
$$k(x_1, x_2) = ck_1(x_1, x_2) (c > 0)$$

2. 커널함수에 양수인 상수를 더한 함수는 커널함수이다.
$$k(x_1, x_2) = k_1(x_1, x_2) + c (c > 0)$$

3. 두 커널함수를 더한 함수는 커널함수이다.
$$k(x_1, x_2) = k_1(x_1, x_2) + k_2(x_1, x_2)$$

4. 두 커널함수를 곱한 함수는 커널함수이다.
$$k(x_1, x_2) = k_1(x_1, x_2)k_2(x_1, x_2)$$

5. 커널함수를 에서 단조증가(monotonically increasing)하는 함수에 적용하면 커널함수이다.
$$k(x_1, x_2) = (k_1(x_1, x_2))^n (n = 1, 2,⋯)$$
$$k(x_1, x_2) = exp(k_1(x_1, x_2))$$
$$k(x_1, x_2) = sigmoid(k_1(x_1, x_2))$$

6. 각각의 커널함수값의 곱도 커널함수이다.
$$k(x_1, x_2) = k_1(x_1, x_1)k_2(x_2, x_2)$$

## 3. 많이 사용되는 커널
다음은 많이 사용되는 커널들인데, 이 커널들은 대부분 기저함수로 변환했을 때 무한대의 자원을 가지는 기저함수가 된다. 따라서 대부분의 비선형성을 처리할 수 있다. (비교를 위해 선형 서포트 벡터 머신의 경우도 추가했다.)
- 선형 서포트 벡터머신
$$k(x_1, x_2) = x_1^Tx_2$$
- 다항 커널(Polynnomial Kernel)
$$k(x_1, x_2) = (\gamma(x_1^Tx_2) + \theta)^d $$
- RBF(Radial Basis Function) 또는 가우시안 커널(Gaussian Kernel)
$$k(x_1, x_2) = exp(-\gamma||x_1-x_2||^2)$$
- 시그모이드 커널(Sigmoid Kernel)
$$k(x_1, x_2) = tanh(\gamma(x_1^Tx_2) + \theta)$$

> 앞에서 사용한 **기저함수**는 $\gamma = 1, \theta = 0, d = 2$인 **다항 커널**임을 알 수 있다.

> 'RBF'에서 유사도는$exp(-\gamma\|\|x_1-x_2\|\|^{2})$ 이다. $x_1 = x_2$이면 $e^0 = 1$이다. 만약 $x_1 \ne x_2$로 떨어져 있으면 거리는 무한대가 된다. $e^{-\infty} = 0$에서 '-' 연산자를 넣는 이유는 exp를 사용하면 그래프가 뒤집히기 때문이다.

### 1) 다항커널
다항커널은 벡터의 내적으로 정의된 커널을 확장하여 만든 커널이다. 

### 2) RBF커널
RBF커널은 가우시안 커널이라고도 한다. RBF커널은 '테일러 정의'로 인해, 테일러 전개가 되면서 무한대인 다항커널이 되낟. 

## 4. Scikit-Learn의 커널 SVM
scikit-learn의 `SVM`클래스는 `kernel`인수를 지정해 **커널을 설정**할 수 있다.
- `kernel = 'inlear'` : 선형 SVM
- `kernel = 'poly'`: 다항 커널
- `kernel = 'rbf' (또는 None)`: RBF커널
- `kernel = 'sigmoid'`: 시그모이드 커널 

```py
# 선형 함수를 제외한 커널 함수
polysvc = SVC(kernel="poly", degree=2, gamma=1, coef0=0).fit(X_xor, y_xor)
rbfsvc = SVC(kernel="rbf").fit(X_xor, y_xor)
sigmoidsvc = SVC(kernel="sigmoid", gamma=2, coef0=2).fit(X_xor, y_xor)
plt.figure(figsize=(8, 12))
plt.subplot(311)
plot_xor(X_xor, y_xor, polysvc, "다항커널 SVC를 사용한 분류 결과")
plt.subplot(312)
plot_xor(X_xor, y_xor, rbfsvc, "RBF커널 SVC를 사용한 분류 결과")
plt.subplot(313)
plot_xor(X_xor, y_xor, sigmoidsvc, "시그모이드커널 SVC를 사용한 분류 결과")
plt.tight_layout()
plt.show()
```
![](/assets/images/Supervised12_2.png)

### 커널 파라미터의 영향
RBF커널(가우시안 커널)을 통해 커널 파라미터의 영향을 확인해보면, gamma($\gamma$)값이 놓아질수록 학습성능은 높아져도 overfitting이 될 수 있다. 따라서 나중에 데이터에 맞게 '하이퍼 파라미터' 튜닝을 해줘야 한다.
```py

plt.figure(figsize=(8, 8))
plt.subplot(221)
plot_xor(X_xor, y_xor, SVC(kernel="rbf", gamma=2).fit(X_xor, y_xor), "RBF SVM (gamma=2)")
plt.subplot(222)
plot_xor(X_xor, y_xor, SVC(kernel="rbf", gamma=10).fit(X_xor, y_xor), "RBF SVM (gamma=10)")
plt.subplot(223)
plot_xor(X_xor, y_xor, SVC(kernel="rbf", gamma=50).fit(X_xor, y_xor), "RBF SVM (gamma=50)")
plt.subplot(224)
plot_xor(X_xor, y_xor, SVC(kernel="rbf", gamma=100).fit(X_xor, y_xor), "RBF SVM (gamma=100)")
plt.tight_layout()
plt.show()
```
![](/assets/images/Supervised12_3.png)